{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19266574",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu\n",
    "---\n",
    "\n",
    "##### Outline\n",
    "Rat simulation:\n",
    "- Motion model (RatInABox)\n",
    "- Environment design (Blender)\n",
    "- Simulated rat vision (ratvision)\n",
    "\n",
    "Vision autoencoder\n",
    "\n",
    "Hippocampus model (RNN):\n",
    "- RNN definition\n",
    "- Data loading\n",
    "- Training\n",
    "\n",
    "Hidden state representations analysis:\n",
    "- Rate maps\n",
    "- Polar maps\n",
    "- Quantitive metrics\n",
    "- Comparison with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0d87d",
   "metadata": {},
   "source": [
    "\n",
    "### *Part 2: Training a Vision Autoencoder*\n",
    "---\n",
    "In this notebook, we will write code to train an **Autoencoder**. An autoencoder is a pair of artificial neural network that compresses information into a low-dimensional embedding. It does so by learning to reconstruct the original information.\n",
    "\n",
    "Researchers use autoencoders to model how neurons might represent visual stimuli in the brain. The visual cortex receives complex images but is able to extract key features, such as edges, motions and shapes, into more compact forms. This non-linear dimensionality reduction process can be somewhat comparable to what an autoencoder achieves.\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Basic Python syntax\n",
    "\n",
    "* Basic understanding of deep learning\n",
    "\n",
    "* We will use the **PyTorch** package for this tutorial. If you have used another DL framework, you will also be fine - simply refer to the documentations\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "\n",
    "* A video recording from the previous part.\n",
    "\n",
    "* The accompanying `utils.py` helper functions.\n",
    "\n",
    "* If you run this locally, you will need a CUDA- or MKL- enabled PyTorch version, and a GPU (or an Apple M-series chip)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ff81b",
   "metadata": {},
   "source": [
    "### **0 Import and install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# install any packages used in the utils.py function here\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8eff9",
   "metadata": {},
   "source": [
    "### **1 Visualize example frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_path = '/Users/marco/Downloads/vrtopc/box/run/exp_dim0.635_fps10_s720_seed01/box_messy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c66925",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_example = frames_path / 'frame0001.png'\n",
    "plt.imshow(np.array(Image.open(frame_example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d45835",
   "metadata": {},
   "source": [
    "### **2 Writing an autoencoder**\n",
    "\n",
    "We begin by coding up our autoencoders.\n",
    "\n",
    "As we mentioned before, an autoencoder is a type of neural network that learns to compress data into a smaller representation (encoding) and then reconstruct it back (decoding).\n",
    "\n",
    "There is no restriction on the structure of the encoder and the decoder, and they need not be symmetric. \n",
    "\n",
    "However, since we are processing image frames, **convolutional layers** will be helpful becuase they can:\n",
    "* capture spatial features like edges, textures, shapes, etc.\n",
    "\n",
    "* preserve local patterns and share weights across image\n",
    "\n",
    "* detect features efficiently regardless of position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58e4a0",
   "metadata": {},
   "source": [
    "First, let's defome some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d733ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = 50 # the number of neurons in the latent space (or number of latent features)\n",
    "img_dim = (128, 64) # (width, height) of the input images\n",
    "grayscale = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, visual_embedding_dim, img_dim, grayscale):\n",
    "        super(VisualEncoder, self).__init__()\n",
    "\n",
    "        self.c_channel = 1 if grayscale else 3\n",
    "        \n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            # WRITE YOUR CODE HERE\n",
    "            \n",
    "            # nn.Conv2d(self.c_channel, 32, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        \n",
    "        # now we flatten and project linearly to the target embedding dimension.\n",
    "        self.threshold_dims = self.get_threshold_dims(img_dim)\n",
    "\n",
    "        self.linear_projection = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(np.prod(self.threshold_dims), visual_embedding_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def get_threshold_dims(self):\n",
    "        # a helper function to obtain the CNN-processed dimension.\n",
    "        w, l = img_dim\n",
    "        with torch.no_grad():\n",
    "            return self.encoder_layers.cpu()(torch.rand(1, self.c_channel, w, l)).shape[1:]\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_conv = self.encoder_layers(img)\n",
    "        emb = self.linear_projection(img_conv)\n",
    "        return emb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a decoder\n",
    "class VisualDecoder(nn.Module):\n",
    "    def __init__(self, visual_embedding_dim, threshold_dims, img_dim, grayscale):\n",
    "        super(VisualDecoder, self).__init__()\n",
    "\n",
    "        self.c_channel = 1 if grayscale else 3\n",
    "        self.threshold_dims = threshold_dims\n",
    "        self.img_dim = img_dim\n",
    "\n",
    "        self.linear_projection = nn.Sequential(\n",
    "            nn.Linear(visual_embedding_dim, np.prod(threshold_dims)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            # WRITE YOUR CODE HERE\n",
    "            \n",
    "            # nn.Unflatten(1, (32, threshold_dim[0], threshold_dim[1])),\n",
    "            # nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            # nn.ReLU(),\n",
    "            # nn.ConvTranspose2d(32, self.c_channel, kernel_size=3, stride=1, padding=1),\n",
    "            \n",
    "            nn.Sigmoid() # projects to (0, 1) scale\n",
    "        )\n",
    "\n",
    "    def forward(self, emb):\n",
    "        emb = self.linear_projection(emb)\n",
    "        emb = emb.reshape(emb.shape[0], self.threshold_dims[0], self.threshold_dims[1], self.threshold_dims[2])\n",
    "        \n",
    "        img_dec = self.decoder_layers(emb)\n",
    "        return img_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779071b",
   "metadata": {},
   "source": [
    "Now that we have defined our encoder and decoder, let's initialise them and see an example of the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a function to preprocess each frame\n",
    "def preprocess_frame(frame, grayscale):\n",
    "    img = Image.open(frame)\n",
    "    if grayscale: \n",
    "        img = img.convert('L')\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    if grayscale:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "    # YOUR CODE HERE: normalise RGB to (0, 1) scale\n",
    "    # img = img / 255.0\n",
    "    \n",
    "    img = np.swapaxes(img, 0, 2) # convert to (3, w, l) if RGB, (1, w, l) if grayscale\n",
    "    return img\n",
    "\n",
    "def preprocess_frame_batch(frames_glob, batch_idxs, grayscale, img_noise=0.):\n",
    "    # preprocess a batch of frames, with indices given by batch_idxs\n",
    "    imgs = np.array([preprocess_frame(frames_glob[idx], grayscale=grayscale) for idx in batch_idxs])\n",
    "    return np.clip(imgs + np.random.normal(0, img_noise, size=imgs.shape), a_min=0, a_max=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f104c18",
   "metadata": {},
   "source": [
    "We will import two pre-defined functions, ```preprocess_frame()``` and ```preprocess_frame_batch()``` from ```utils.py```.\n",
    "\n",
    "The ```preprocess_frame``` accepts the path of a frame, then converts it to RGB values and normalise it 1 so that we have a ```(width, length, channels)``` array of numbers between 0 and 1.\n",
    "\n",
    "Optionally, the frame can be converted to grayscale, so that the channel dimension is 1. It can also add Gaussian noise to the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess_frame, preprocess_frame_batch\n",
    "\n",
    "encoder = VisualEncoder(\n",
    "    visual_embedding_dim = 50, # the number of neurons in the latent space (or number of latent features)\n",
    "    img_dim = (128, 64), # (width, height) of the input images\n",
    "    grayscale = True\n",
    ").to(device)\n",
    "\n",
    "decoder = VisualDecoder(\n",
    "    visual_embedding_dim = 50,\n",
    "    threshold_dims = encoder.threshold_dims,\n",
    "    img_dim = (128, 64),\n",
    "    grayscale = True\n",
    ").to(device)\n",
    "\n",
    "# visualise an example\n",
    "with torch.no_grad():\n",
    "    frame_example_img = preprocess_frame(frame_example, grayscale = True)\n",
    "    frame_example_recon = decoder(encoder(torch.as_tensor(frame_example_img, dtype=torch.float32).to(device)))\n",
    "    \n",
    "plt.imshow(np.swapaxes(frame_example_recon.cpu().numpy(), 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32823568",
   "metadata": {},
   "source": [
    "This looks like just noise. This is because the autoencoder is untrained. But the good news is: at least we got the dimensions correct!\n",
    "\n",
    "Now, let's train the pair of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c2399",
   "metadata": {},
   "source": [
    "### **3 Training an autoencoder**\n",
    "\n",
    "The process of training a neural network is well established. In general, we follow these steps:\n",
    "\n",
    "* **Collect and pre-process data**: in the case of image frames, we need to consider normalising the RGB values.\n",
    "\n",
    "* **Split the dataset** into train, test (and optionally validation) sets\n",
    "\n",
    "* **Initialise model weights**, which we have just done!\n",
    "\n",
    "* **Train the model**. This includes a forward pass, computing loss, backpropagation and update weights\n",
    "\n",
    "* Where needed, **validate** on the validation set to tune hyperparameters.\n",
    "\n",
    "* **Test** the trained model on unseen data to evaluate performance.\n",
    "\n",
    "* Once the model has reached satisfactory performance, it is ready for **deployment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split dataset\n",
    "batch_size = 2048\n",
    "test_set_proportion = 0.05 # 5%\n",
    "\n",
    "# create a glob list of all file paths\n",
    "all_frames_glob = [png for png in frames_path.glob('*.png')]\n",
    "train_idxs, test_idxs = train_test_split(\n",
    "    np.arange(len(all_frames_glob)), test_size=test_set_proportion, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc27ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can define the train and test functions\n",
    "def train_epoch(\n",
    "    enc, dec,\n",
    "    train_idxs, batch_size, all_frames_glob,\n",
    "    loss_fn, optim,\n",
    "    grayscale, scheduler=None, img_noise=0.0\n",
    "):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    epoch_loss = 0\n",
    "    n_train_batches = len(train_idxs) // batch_size\n",
    "    \n",
    "    for batch_i in range(n_train_batches):\n",
    "        batch_idxs = train_idxs[batch_i*batch_size : (batch_i + 1)*batch_size]\n",
    "        imgs = preprocess_frame_batch(all_frames_glob, batch_idxs, grayscale=grayscale, img_noise=img_noise)\n",
    "        imgs = torch.as_tensor(imgs, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # YOUR CODE HERE: forward pass\n",
    "        # imgs_recon = dec(enc(imgs))\n",
    "        # loss = loss_fn(imgs, imgs_recon)\n",
    "        \n",
    "        # YOUR CODE HERE: backward pass and optimisation step\n",
    "        # optim.zero_grad()\n",
    "        # loss.backward()\n",
    "        # optim.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "            \n",
    "    return epoch_loss / n_train_batches\n",
    "\n",
    "def test_epoch(\n",
    "    enc, dec,\n",
    "    test_idxs, batch_size, all_frames_glob,\n",
    "    loss_fn,\n",
    "    grayscale\n",
    "):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    epoch_loss = 0\n",
    "    n_test_batches = len(test_idxs) // batch_size\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_i in range(n_test_batches):\n",
    "            batch_idxs = test_idxs[batch_i*batch_size : (batch_i + 1)*batch_size]\n",
    "            imgs = preprocess_frame_batch(all_frames_glob, batch_idxs, grayscale=grayscale, img_noise=0.0)\n",
    "            imgs = torch.as_tensor(imgs, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # YOUR CODE HERE: forward pass\n",
    "            # imgs_recon = dec(enc(imgs))\n",
    "            # loss = loss_fn(imgs, imgs_recon)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / n_test_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, putting it all together: the training loop\n",
    "\n",
    "n_epochs = 10\n",
    "loss_fn = nn.L1Loss()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optim = torch.optim.Adam(itertools.chain(encoder.parameters(), decoder.parameters()), lr=learning_rate)\n",
    "\n",
    "# optional: use a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        encoder, decoder, train_idxs, batch_size, all_frames_glob,\n",
    "        loss_fn, optim, grayscale=True, scheduler=scheduler\n",
    "    )\n",
    "    test_loss = test_epoch(\n",
    "        encoder, decoder, test_idxs, batch_size, all_frames_glob,\n",
    "        loss_fn, grayscale=True\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        frame_example_recon = decoder(encoder(torch.as_tensor(frame_example_img, dtype=torch.float32).to(device)))\n",
    "    \n",
    "        plt.imshow(np.swapaxes(frame_example_recon.cpu().numpy(), 0, 2))\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486fb19",
   "metadata": {},
   "source": [
    "### **What have we achieved in this tutorial?**\n",
    "\n",
    "We trained a pair of autoencoders, that can compress images into a latent vector.\n",
    "\n",
    "With a biological constraint (such as non-negative, or sigmoid), then much like the visual cortex, we can interpret the latent vector for each frame as population activity of visual neurons encoding that visual scene.\n",
    "\n",
    "These encodings of the visual scene can now be readily fed into a **Recurrent Neural Network** to generate spatial cell tunings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
