{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613c077f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Modelling hippocampal neurons of animals navigating in VR with Recurrent Neural Networks\n",
    "\n",
    "##### *Tutorial, COS 2025*\n",
    "### *Part 3: Training an RNN*\n",
    "##### made by: Daniel Liu, Marco Abrate, UCL\n",
    "---\n",
    "In this notebook, we will write code to define the **RNN**. \n",
    "\n",
    "Recurrent Neural Networks (RNNs) are neural models designed to process sequential data by retaining memory over time. Specifically in this tutorial, we train an RNN to perform a **next-step prediction task**.\n",
    "\n",
    "What is next-step prediction and why?\n",
    "\n",
    "\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Completed Notebook 1\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "\n",
    "* All frames, processed into embeddings in ```.npy``` file, from the Autoencoder we trained in the last tutorial.\n",
    "\n",
    "* Trajectory file, including speed and rotational velocity at each discritised time step.\n",
    "\n",
    "* The accompanying `utils.py` helper functions.\n",
    "\n",
    "* If you run this locally, you will need a CUDA- or MKL- enabled PyTorch version, and a GPU (or an Apple M-series chip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14611fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# install any packages used in the utils.py function here\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embeddings and trajectory data\n",
    "train_embeddings = None\n",
    "train_speed = None\n",
    "train_rot_vels = None\n",
    "\n",
    "assert train_embeddings.shape[0] == train_speed.shape[0] == train_rot_vels.shape[0]\n",
    "\n",
    "visual_embedding_dim = train_embeddings.shape[-1]\n",
    "motion_signal_dim = train_speed.shape[-1] + train_rot_vels.shape[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ff46f",
   "metadata": {},
   "source": [
    "#### Defining an RNN\n",
    "\n",
    "Let's define a recurrent neural network. This network will use a pre-defined ```RNNModule``` class, which we have provided, that employs Sigmoidal activation functions when projecting inputs to hidden states. The hidden states are then projected to predict the next sensory state via a linear layer. \n",
    "\n",
    "The sigmoidal function compresses the hidden states to between 0 and 1, which can be interpreted as the (scaled) activity of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathIntegrationRNN(nn.Module):\n",
    "    '''\n",
    "    This class implements a simple RNN that uses current step sensory input, motion signals,\n",
    "    and the previous hidden state to predict the next step sensory input.\n",
    "    The RNN is expected to learn to integrate the sensory inputs over time, effectively simulating\n",
    "    path integration.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device):\n",
    "        super(PathIntegrationRNN, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.rnn = RNNModule(n_inputs =input_dim, n_hidden=hidden_dim, nonlinearity='sigmoid', device=device)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # COMPLETE THE CODE HERE: \n",
    "        # x should be of shape (batch_size, seq_length, input_dim)\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # out, hidden = self.rnn(x, hidden)\n",
    "        # out = self.fc(out)\n",
    "        # return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229dde1",
   "metadata": {},
   "source": [
    "Note that our input dimension would be the both the visual embedding dimension and the motion signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = PathIntegrationRNN(input_dim=visual_embedding_dim + motion_signal_dim,\n",
    "                         hidden_dim=50,\n",
    "                         output_dim=visual_embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaaa7f3",
   "metadata": {},
   "source": [
    "#### Defining a DataLoader\n",
    "\n",
    "To allow our RNN to perform the next-state prediction task, we need to use a **Dataset** and **DataLoader**. The **Dataset** prepares sensory embeddings as well as motion signals to batches of paired inputs and labels; the **DataLoader** sequentially generates these batches during training and testing.\n",
    "\n",
    "The Dataset and Dataloader classes will inherit PyTorch's built-in ```torch.utils.data.Dataset``` and ```torch.utils.data.DataLoader``` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d21fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embs, vels, rot_vels, hds, thetas, pos, tsteps=9, n_future_pred=1):\n",
    "        '''\n",
    "        The initialisation function for the SensoryDataset class.\n",
    "        At initialisation, all embeddings are converted to tensors.\n",
    "        Args:\n",
    "            embs: The visual embeddings of shape (N, D)\n",
    "            vels: The speed signals of shape (N, 1)\n",
    "            rot_vels: The rotational velocities of shape (N, 1)\n",
    "            hds: The headings of shape (N, 1)\n",
    "            thetas: The headings of shape (N, 1)\n",
    "            tsteps: The number of time steps for each batch. \n",
    "                    By default, this is set to 9 i.e. we use the sensory input from steps 1 to 9\n",
    "            n_future_pred: The number of steps into the future to predict.\n",
    "                    By default, this is set to 1 i.e. we predict the sensory input at steps 2 to 10            \n",
    "        '''\n",
    "        \n",
    "        self.embs = torch.tensor(embs, dtype=torch.float32)\n",
    "        self.vels = torch.tensor(vels, dtype=torch.float32)\n",
    "        self.rot_vels = torch.tensor(rot_vels, dtype=torch.float32)\n",
    "        self.hds = torch.tensor(hds, dtype=torch.float32)\n",
    "        self.thetas = torch.tensor(thetas, dtype=torch.float32)\n",
    "        self.pos = torch.tensor(pos, dtype=torch.float32)\n",
    "        \n",
    "        self.tsteps = tsteps\n",
    "        self.n_future_pred = n_future_pred\n",
    "        \n",
    "        del embs, vels, rot_vels, hds, thetas # free up memory\n",
    "    \n",
    "    def __len__(self):\n",
    "        # COMPLETE THE CODE HERE: how many samples are in the dataset?\n",
    "        return self.embs.shape[0] // self.tsteps - self.n_future_pred\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        This function returns a batch of sensory inputs and the corresponding future sensory inputs.\n",
    "        Args:\n",
    "            idx: The index of the sample to return.\n",
    "        Returns:\n",
    "            A tuple containing:\n",
    "                - sensory_inputs: A tensor of shape (tsteps, input_dim) containing the sensory inputs\n",
    "                - future_sensory_inputs: A tensor of shape (n_future_pred, input_dim) containing the future sensory inputs\n",
    "        '''\n",
    "        embs, vels, rot_vels, pos, hds, theta, embs_labels = [], [], [], [], [], [], []\n",
    "        embs = self.embs[idx * self.tsteps:(idx + 1) * self.tsteps]\n",
    "        \n",
    "        start_idx, end_idx = idx * self.tsteps, (idx + 1) * self.tsteps\n",
    "        \n",
    "        for future_step in range(self.n_future_pred):\n",
    "            # COMPLETE THE CODE HERE: \n",
    "            # Get the sensory inputs and future sensory inputs for the given index\n",
    "            pass\n",
    "        \n",
    "            # vels.append(self.vels[:, start_idx + future_step: end_idx + future_step])\n",
    "            # rot_vels.append(self.rot_vels[:, start_idx + future_step: end_idx + future_step])\n",
    "            # pos.append(self.pos[:, start_idx + future_step: end_idx + future_step])\n",
    "            # hds.append(self.hds[:, start_idx + future_step: end_idx + future_step])\n",
    "            # theta.append(self.thetas[:, start_idx + future_step: end_idx + future_step])\n",
    "            \n",
    "            # embs_labels.append(embs[:, start_idx + future_step + 1: end_idx + future_step + 1])\n",
    "        \n",
    "        vels, rot_vels, pos, hds, theta, embs_labels = torch.vstack(vels, dim=1), \\\n",
    "                                                        torch.vstack(rot_vels, dim=1), \\\n",
    "                                                        torch.vstack(pos, dim=1), \\\n",
    "                                                        torch.vstack(hds, dim=1), \\\n",
    "                                                        torch.vstack(theta, dim=1), \\\n",
    "                                                        torch.vstack(embs_labels, dim=1) \n",
    "            \n",
    "        \n",
    "        return embs, vels, rot_vels, pos, hds, theta, embs_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3a273",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
