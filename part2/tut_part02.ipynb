{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19266574",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals <br> navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; University College London (UCL)\n",
    "---\n",
    "\n",
    "#### Outline\n",
    "**Part 1: Rat simulation in 3D**\n",
    "- Motion model with `RatInABox`\n",
    "\n",
    "- Environment design\n",
    "\n",
    "- Simulated rat vision with `ratvision`\n",
    "\n",
    "**Part 2: Vision autoencoder**\n",
    "\n",
    "**Part 3: Hippocampus model with RNN**\n",
    "\n",
    "**Part 4: Hidden state representations analysis**\n",
    "- Rate maps\n",
    "\n",
    "- Polar maps\n",
    "\n",
    "- Quantitive metrics\n",
    "\n",
    "- Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0d87d",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 2: Vision Autoencoder**\n",
    "\n",
    "In this notebook, we will write code in **Pytorch** to train an **autoencoder** that works with images. An autoencoder is a pair of artificial neural networks that compresses information into a low-dimensional embedding through the first module (aka encoder) and reconstructs it to its original form through the second module (aka decoder).\n",
    "\n",
    "Neuroscientists use vision autoencoders to model how neurons might represent visual stimuli in the brain [1]. The visual cortex receives complex images and it is able to extract key features into more compact forms (low-dimensional embedding). This non-linear dimensionality reduction process, along with the reconstruction of the original image, can be compared to an autoencoder.\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- video recordings from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ff81b",
   "metadata": {},
   "source": [
    "### 0. Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee49780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:13.070377Z",
     "iopub.status.busy": "2025-06-25T11:47:13.070273Z",
     "iopub.status.idle": "2025-06-25T11:47:14.662731Z",
     "shell.execute_reply": "2025-06-25T11:47:14.661534Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install numpy\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:14.665093Z",
     "iopub.status.busy": "2025-06-25T11:47:14.664965Z",
     "iopub.status.idle": "2025-06-25T11:47:15.996408Z",
     "shell.execute_reply": "2025-06-25T11:47:15.995671Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8eff9",
   "metadata": {},
   "source": [
    "### 1. Visualize example frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcac369",
   "metadata": {},
   "source": [
    "We load vision data from 26 trials of length 720 seconds at 10 Hz, for a total of 187,200 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19f9c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.067540Z",
     "iopub.status.busy": "2025-06-25T11:47:16.067430Z",
     "iopub.status.idle": "2025-06-25T11:47:16.069753Z",
     "shell.execute_reply": "2025-06-25T11:47:16.069415Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_DIM = (64, 128) # (height, width) of the input images\n",
    "GS = True # whether to use grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bea2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.000340Z",
     "iopub.status.busy": "2025-06-25T11:47:16.000159Z",
     "iopub.status.idle": "2025-06-25T11:47:16.005033Z",
     "shell.execute_reply": "2025-06-25T11:47:16.004663Z"
    }
   },
   "outputs": [],
   "source": [
    "d = '../data/vision_ae_data'\n",
    "\n",
    "trial_paths = [p for p in Path(d).iterdir() if 'exp' in p.name]\n",
    "\n",
    "trial_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c66925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.006027Z",
     "iopub.status.busy": "2025-06-25T11:47:16.005923Z",
     "iopub.status.idle": "2025-06-25T11:47:16.064882Z",
     "shell.execute_reply": "2025-06-25T11:47:16.063931Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_example = trial_paths[0] / 'box_messy' / 'frame0001.png'\n",
    "plt.imshow(np.array(Image.open(frame_example)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d45835",
   "metadata": {},
   "source": [
    "### 2. Load frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f104c18",
   "metadata": {},
   "source": [
    "The `preprocess_frame` function accepts the path to a frame, then converts it to RGB (or grayscale) values and normalises it 1. The output is a `(channels, height, width)` array of numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5dfe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.070700Z",
     "iopub.status.busy": "2025-06-25T11:47:16.070603Z",
     "iopub.status.idle": "2025-06-25T11:47:16.073318Z",
     "shell.execute_reply": "2025-06-25T11:47:16.073004Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, grayscale):\n",
    "    img = Image.open(frame)\n",
    "    if grayscale: \n",
    "        img = img.convert('L')\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "\n",
    "    # normalise RGB to (0, 1) scale\n",
    "    img = img / 255.\n",
    "    \n",
    "    if grayscale:\n",
    "        img = img[None, ...] # (1, h, w) if grayscale\n",
    "    else:\n",
    "        img = np.moveaxis(img, -1, 0) # reshape to (3, h, w) if RGB\n",
    "\n",
    "    return img\n",
    "\n",
    "def preprocess_frame_batch(all_frames, batch_indices, grayscale):\n",
    "    # preprocess a batch of frames\n",
    "    imgs = np.array([\n",
    "        preprocess_frame(all_frames[idx], grayscale)\n",
    "        for idx in batch_indices\n",
    "    ])\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938aa7f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.074165Z",
     "iopub.status.busy": "2025-06-25T11:47:16.074060Z",
     "iopub.status.idle": "2025-06-25T11:47:16.487495Z",
     "shell.execute_reply": "2025-06-25T11:47:16.486951Z"
    }
   },
   "outputs": [],
   "source": [
    "globs = [(tp/'box_messy').glob('*.png') for tp in trial_paths]\n",
    "all_frames = [f for g in globs for f in g]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49ce81",
   "metadata": {},
   "source": [
    "We split the frames into a set used for training and a set used to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5488d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.490968Z",
     "iopub.status.busy": "2025-06-25T11:47:16.490845Z",
     "iopub.status.idle": "2025-06-25T11:47:16.497783Z",
     "shell.execute_reply": "2025-06-25T11:47:16.497287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "BATCH_SIZE = 32\n",
    "TEST_SET_PROP = 0.075 # 7.5%\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(all_frames)), test_size=TEST_SET_PROP,\n",
    "    shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b66795",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:16.498872Z",
     "iopub.status.busy": "2025-06-25T11:47:16.498765Z",
     "iopub.status.idle": "2025-06-25T11:47:51.777624Z",
     "shell.execute_reply": "2025-06-25T11:47:51.776596Z"
    }
   },
   "outputs": [],
   "source": [
    "train_imgs = preprocess_frame_batch(\n",
    "    all_frames, train_indices, grayscale=GS\n",
    ")\n",
    "test_imgs = preprocess_frame_batch(\n",
    "    all_frames, test_indices, grayscale=GS\n",
    ")\n",
    "\n",
    "print(f\"Train set shape:\\t{train_imgs.shape}\")\n",
    "print(f\"Test set shape:\\t\\t{test_imgs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b9d1ce",
   "metadata": {},
   "source": [
    "We define the `Dataloader`, which is a special Pytorch class helpful to iterate over samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df24405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:51.780661Z",
     "iopub.status.busy": "2025-06-25T11:47:51.780157Z",
     "iopub.status.idle": "2025-06-25T11:47:51.783462Z",
     "shell.execute_reply": "2025-06-25T11:47:51.783137Z"
    }
   },
   "outputs": [],
   "source": [
    "from autoencoder.datasets import UnlabeledDataset\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    UnlabeledDataset(torch.from_numpy(train_imgs)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    UnlabeledDataset(torch.from_numpy(test_imgs)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2dbc3",
   "metadata": {},
   "source": [
    "### 3. Vision autoencoder definition\n",
    "\n",
    "As we mentioned before, an autoencoder is a type of neural network that learns to compress data into a smaller representation (aka embedding) &mdash; through an encoder `VisionEncoder` &mdash; and to reconstruct it &mdash; through a decoder `VisionDecoder`. There is no restriction on the structure of the encoder and the decoder, and they don't need to be symmetric. \n",
    "\n",
    "However, since we are processing image frames, **convolutional layers** will be helpful becuase they can:\n",
    "- capture spatial features like edges, textures, shapes, etc.\n",
    "\n",
    "- preserve local patterns and share weights across images\n",
    "\n",
    "- detect features efficiently regardless of position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e4d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:51.784417Z",
     "iopub.status.busy": "2025-06-25T11:47:51.784310Z",
     "iopub.status.idle": "2025-06-25T11:47:51.787888Z",
     "shell.execute_reply": "2025-06-25T11:47:51.787559Z"
    }
   },
   "outputs": [],
   "source": [
    "from autoencoder.vision_ae import VisionEncoder, VisionDecoder\n",
    "\n",
    "# YOUR CODE HERE (1)\n",
    "KERNEL_SIZES = None # kernel sizes for the convolutional layers\n",
    "KERNEL_STRIDES = None # strides\n",
    "CHANNELS = None # number of channels\n",
    "\n",
    "EMBEDDING_DIM = None # the number of neurons in the hidden state (aka embedding dimension)\n",
    "\n",
    "img_dim_out = IMG_DIM\n",
    "\n",
    "print('Need to make sure all numbers are INTEGERS!\\n')\n",
    "print(f'Input dimension:\\t\\t{1 if GS else 3}x {IMG_DIM}')\n",
    "for i in range(len(KERNEL_SIZES)):\n",
    "    ksize = KERNEL_SIZES[i]\n",
    "    stride = KERNEL_STRIDES[i]\n",
    "\n",
    "    img_dim_out = [\n",
    "        (img_dim_out[i] - ksize[i])/stride + 1\n",
    "        for i in range(len(IMG_DIM))\n",
    "    ]\n",
    "    \n",
    "    print(f'Intermediate dimension {i+1}:\\t{CHANNELS[i]}x {img_dim_out}')\n",
    "\n",
    "print(f'Flattens to:\\t\\t\\t{np.prod(img_dim_out)*CHANNELS[-1]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e1855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:51.788765Z",
     "iopub.status.busy": "2025-06-25T11:47:51.788659Z",
     "iopub.status.idle": "2025-06-25T11:47:51.863058Z",
     "shell.execute_reply": "2025-06-25T11:47:51.862473Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = VisionEncoder(\n",
    "    vision_embedding_dim = EMBEDDING_DIM,\n",
    "    img_dim = IMG_DIM,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(DEVICE)\n",
    "\n",
    "decoder = VisionDecoder(\n",
    "    vision_embedding_dim = EMBEDDING_DIM,\n",
    "    img_dim_out = encoder.img_dim_out,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e312c44",
   "metadata": {},
   "source": [
    "Now that we have defined our encoder and decoder, let's see an example of the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf060b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:51.865237Z",
     "iopub.status.busy": "2025-06-25T11:47:51.865126Z",
     "iopub.status.idle": "2025-06-25T11:47:52.072972Z",
     "shell.execute_reply": "2025-06-25T11:47:52.072127Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualise examples\n",
    "with torch.no_grad():\n",
    "    for example_batch in dataloader_test:\n",
    "        example_batch = example_batch[np.random.choice(len(example_batch), size=3, replace=False)]\n",
    "        example_batch = example_batch.to(DEVICE)\n",
    "        example_batch_recon = decoder(encoder(example_batch))\n",
    "\n",
    "        fig, axs = plt.subplots(len(example_batch), 2, figsize=(6, 1.5*len(example_batch)))\n",
    "        axs.flat[0].set_title('Original Images')\n",
    "        axs.flat[1].set_title('Reconstructed Images')\n",
    "        for i, (frame_example_img, frame_example_recon) in enumerate(zip(example_batch, example_batch_recon)):\n",
    "            axs[i, 0].imshow(frame_example_img.cpu().numpy().squeeze(), cmap='gray')\n",
    "            axs[i, 1].imshow(frame_example_recon.cpu().numpy().squeeze(), cmap='gray')\n",
    "            axs[i, 0].axis('off')\n",
    "            axs[i, 1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32823568",
   "metadata": {},
   "source": [
    "This looks like just noise. This is because the autoencoder is not trained yet. But the good news is: at least we got the dimensions correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c2399",
   "metadata": {},
   "source": [
    "### 4. Autoencoder training\n",
    "\n",
    "* **Train the model** by comparing its reconstruction to the original image through a self-supervised task.\n",
    "\n",
    "* If needed, **validate** on the validation set to tune hyperparameters.\n",
    "\n",
    "* **Test** the trained model on unseen data to evaluate performance.\n",
    "\n",
    "* Once the model has reached satisfactory performance, embed novel vision data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8ef85",
   "metadata": {},
   "source": [
    "We start by defining the train and test loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc27ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:52.074407Z",
     "iopub.status.busy": "2025-06-25T11:47:52.074288Z",
     "iopub.status.idle": "2025-06-25T11:47:52.078104Z",
     "shell.execute_reply": "2025-06-25T11:47:52.077759Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    enc, dec,\n",
    "    dataloader_train,\n",
    "    loss_fn, optimizer\n",
    "):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(DEVICE)\n",
    "        \n",
    "        # YOUR CODE HERE (2)\n",
    "        # forward and backward passes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "        epoch_loss += loss.detach().item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader_train)\n",
    "\n",
    "def test_epoch(\n",
    "    enc, dec,\n",
    "    dataloader_test,\n",
    "    loss_fn,\n",
    "):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_test:\n",
    "            batch = batch.to(DEVICE)\n",
    "            \n",
    "            # YOUR CODE HERE (3)\n",
    "            # forward pass\n",
    "            raise NotImplementedError\n",
    "            \n",
    "            epoch_loss += loss.detach().item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be98920",
   "metadata": {},
   "source": [
    "We finalize by setting some important parameters &mdash; such as the number of epochs and the learning rate &mdash; and define the loss function, the optimizer, and (optionally) a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bc65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:52.079047Z",
     "iopub.status.busy": "2025-06-25T11:47:52.078945Z",
     "iopub.status.idle": "2025-06-25T11:47:52.649880Z",
     "shell.execute_reply": "2025-06-25T11:47:52.649126Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 5 # 200 to perform a full training\n",
    "learning_rate = 1e-4\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    itertools.chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd6c4c",
   "metadata": {},
   "source": [
    "We train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1d8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T11:47:52.653360Z",
     "iopub.status.busy": "2025-06-25T11:47:52.653132Z",
     "iopub.status.idle": "2025-06-25T12:20:08.776216Z",
     "shell.execute_reply": "2025-06-25T12:20:08.775480Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_every = 5 # how often to plot examples\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        encoder, decoder,\n",
    "        dataloader_train,\n",
    "        loss_fn, optimizer\n",
    "    )\n",
    "    test_loss = test_epoch(\n",
    "        encoder, decoder,\n",
    "        dataloader_test,\n",
    "        loss_fn\n",
    "    )\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "    # visualize examples\n",
    "    if epoch == 0 or (epoch+1)%plot_every == 0:\n",
    "        with torch.no_grad():\n",
    "            for example_batch in dataloader_test:\n",
    "                example_batch = example_batch[\n",
    "                    np.random.choice(len(example_batch), size=3, replace=False)\n",
    "                ]\n",
    "                example_batch = example_batch.to(DEVICE)\n",
    "                example_batch_recon = decoder(encoder(example_batch))\n",
    "\n",
    "                fig, axs = plt.subplots(len(example_batch), 2, figsize=(6, 1.5*len(example_batch)))\n",
    "                axs.flat[0].set_title('Original Images')\n",
    "                axs.flat[1].set_title('Reconstructed Images')\n",
    "                for i, (frame_example, frame_recon) in enumerate(zip(example_batch, example_batch_recon)):\n",
    "                    axs[i, 0].imshow(frame_example.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "                    axs[i, 1].imshow(frame_recon.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "                    axs[i, 0].axis('off')\n",
    "                    axs[i, 1].axis('off')\n",
    "                plt.show()\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ab7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(range(1, len(train_loss_list)+1), train_loss_list, label='Train Loss', color='blue')\n",
    "plt.plot(range(1, len(test_loss_list)+1), test_loss_list, label='Test Loss', color='orange')\n",
    "plt.xticks(range(1, len(train_loss_list)+1))\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a260cacb",
   "metadata": {},
   "source": [
    "Train and test loss for a ful training process.\n",
    "\n",
    "![Train and test loss](train_test_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280966ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment this to save your new model's weights\n",
    "\n",
    "# torch.save(encoder.state_dict(), 'encoder.pth')\n",
    "# torch.save(decoder.state_dict(), 'decoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6b9b3",
   "metadata": {},
   "source": [
    "### 5. Embed rat vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69760de6",
   "metadata": {},
   "source": [
    "We load a previously saved model, which was trained by setting the number of epochs to 100 in the previous step. To embed new frames we would just need the **Encoder**, but we also load the **Decoder** to show examples of reconstruction from a performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "encoder.load_state_dict(torch.load('encoder.pth', weights_only=True, map_location=DEVICE))\n",
    "decoder.load_state_dict(torch.load('decoder.pth', weights_only=True, map_location=DEVICE))\n",
    "\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d451c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_batch in dataloader_test:\n",
    "    example_batch = example_batch[\n",
    "        np.random.choice(len(example_batch), size=5, replace=False)\n",
    "    ]\n",
    "    example_batch = example_batch.to(DEVICE)\n",
    "    example_batch_recon = decoder(encoder(example_batch))\n",
    "\n",
    "    fig, axs = plt.subplots(len(example_batch), 2, figsize=(6, 1.5*len(example_batch)))\n",
    "    axs.flat[0].set_title('Original Images')\n",
    "    axs.flat[1].set_title('Reconstructed Images')\n",
    "    for i, (frame_example, frame_recon) in enumerate(zip(example_batch, example_batch_recon)):\n",
    "        axs[i, 0].imshow(frame_example.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "        axs[i, 1].imshow(frame_recon.detach().cpu().numpy().squeeze(), cmap='gray')\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 1].axis('off')\n",
    "    plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26ebe1",
   "metadata": {},
   "source": [
    "Load rat vision data and embed it using a trained **Encoder**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cc65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = '../data/adult'\n",
    "trial_paths = sorted([p for p in Path(d).iterdir() if 'exp' in p.name])\n",
    "\n",
    "for tp in trial_paths:\n",
    "    print(tp)\n",
    "\n",
    "    frame_paths = sorted([f for f in (tp/'box_messy').glob('*.png')])\n",
    "\n",
    "    frames = preprocess_frame_batch(\n",
    "        frame_paths, np.arange(len(frame_paths)), grayscale=GS\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        UnlabeledDataset(torch.from_numpy(frames)),\n",
    "        batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "    \n",
    "    batch_emb_all = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(DEVICE)\n",
    "            \n",
    "            batch_emb = encoder(batch)\n",
    "            \n",
    "            batch_emb_all.append(batch_emb.cpu().numpy())\n",
    "\n",
    "    batch_emb_all = np.concatenate(batch_emb_all, axis=0)\n",
    "\n",
    "    print(batch_emb_all.shape)\n",
    "    np.save(tp/'vision_embeddings.npy', batch_emb_all)\n",
    "    print(f'Saved embeddings to {tp/\"vision_embeddings.npy\"}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486fb19",
   "metadata": {},
   "source": [
    "### **Summary: what have we achieved?**\n",
    "\n",
    "We trained a Vision Autoencoder that can compress images into **embeddings** &mdash; namely smaller representations of the inputs.\n",
    "\n",
    "These embeddings of the visual scene can be though of as inputs to the hippocampus &mdash; which we model as a **Predictive Recurrent Neural Network** in the next part &mdash; thought to transform egocentric (i.e. self-centred) information into a coherent allocentric (i.e. world-centred) representation. This is crucial to support flexible navigation strategies such as taking shortcuts or planning routes to unseen locations [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1cc767",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] G. W. Lindsay. Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future. J Cogn Neurosci, 33(10):2017–2031, 2021.\n",
    "\n",
    "[2] E. C. Tolman. Cognitive maps in rats and men. Psychological review, 55(4):189, 1948."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d884a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
