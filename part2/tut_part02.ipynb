{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19266574",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu\n",
    "---\n",
    "\n",
    "##### Outline\n",
    "Rat simulation:\n",
    "- Motion model (RatInABox)\n",
    "- Environment design (Blender)\n",
    "- Simulated rat vision (ratvision)\n",
    "\n",
    "Vision autoencoder\n",
    "\n",
    "Hippocampus model (RNN):\n",
    "- RNN definition\n",
    "- Data loading\n",
    "- Training\n",
    "\n",
    "Hidden state representations analysis:\n",
    "- Rate maps\n",
    "- Polar maps\n",
    "- Quantitive metrics\n",
    "- Comparison with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0d87d",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 2: Training a Vision Autoencoder**\n",
    "\n",
    "In this notebook, we will write code to train an **Autoencoder**. An autoencoder is a pair of artificial neural networks that compresses information into a low-dimensional embedding through the first module (aka encoder) and reconstructs it to its original form through the second module (aka decoder).\n",
    "\n",
    "Neuroscientists use vision autoencoders to model how neurons might represent visual stimuli in the brain. The visual cortex receives complex images and it is able to extract key features - such as edges, motions and shapes - into more compact forms (low-dimensional embedding). This non-linear dimensionality reduction process, along with the reconstruction of the original image, can be compared to an autoencoder.\n",
    "\n",
    "We will use the **PyTorch** package for this tutorial.\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- video recordings from part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ff81b",
   "metadata": {},
   "source": [
    "### **0. Install and import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee49780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:31.761628Z",
     "iopub.status.busy": "2025-06-24T18:27:31.761500Z",
     "iopub.status.idle": "2025-06-24T18:27:33.318161Z",
     "shell.execute_reply": "2025-06-24T18:27:33.317203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from triton==3.3.1->torch) (78.1.1)\n",
      "Requirement already satisfied: numpy in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: scikit-learn in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: matplotlib in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/marco/miniconda3/envs/tutorial/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6512",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:33.321613Z",
     "iopub.status.busy": "2025-06-24T18:27:33.321494Z",
     "iopub.status.idle": "2025-06-24T18:27:34.652703Z",
     "shell.execute_reply": "2025-06-24T18:27:34.652249Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8eff9",
   "metadata": {},
   "source": [
    "### **1. Visualize example frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bea2a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:34.654559Z",
     "iopub.status.busy": "2025-06-24T18:27:34.654392Z",
     "iopub.status.idle": "2025-06-24T18:27:34.658978Z",
     "shell.execute_reply": "2025-06-24T18:27:34.658661Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "d = '/media/data/vrtopc/box/run' # '/Users/marco/Downloads/vrtopc/box/run'\n",
    "\n",
    "trial_paths = [p for p in Path(d).iterdir() if 'exp' in p.name]\n",
    "\n",
    "trial_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c66925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:34.660003Z",
     "iopub.status.busy": "2025-06-24T18:27:34.659904Z",
     "iopub.status.idle": "2025-06-24T18:27:34.717609Z",
     "shell.execute_reply": "2025-06-24T18:27:34.716534Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "frame_example = trial_paths[0] / 'box_messy' / 'frame0001.png'\n",
    "plt.imshow(np.array(Image.open(frame_example)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19f9c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "IMG_DIM = (64, 128) # (height, width) of the input images\n",
    "GS = True # whether to use grayscale images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d45835",
   "metadata": {},
   "source": [
    "### **2. Load frames**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f104c18",
   "metadata": {},
   "source": [
    "The ```preprocess_frame``` accepts the path of a frame, then converts it to RGB values and normalise it 1 so that we have a ```(channels, height, width)``` array of numbers between 0 and 1.\n",
    "\n",
    "Optionally, the frame can be converted to grayscale, so that the channel dimension is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5dfe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:34.733311Z",
     "iopub.status.busy": "2025-06-24T18:27:34.733201Z",
     "iopub.status.idle": "2025-06-24T18:27:34.735891Z",
     "shell.execute_reply": "2025-06-24T18:27:34.735592Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def preprocess_frame(frame, grayscale):\n",
    "    img = Image.open(frame)\n",
    "    if grayscale: \n",
    "        img = img.convert('L')\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "\n",
    "    # normalise RGB to (0, 1) scale\n",
    "    img = img / 255.\n",
    "    \n",
    "    if grayscale:\n",
    "        img = img[None, ...] # (1, h, w) if grayscale\n",
    "    else:\n",
    "        img = np.moveaxis(img, -1, 0) # reshape to (3, h, w) if RGB\n",
    "\n",
    "    return img\n",
    "\n",
    "def preprocess_frame_batch(all_frames, batch_indices, grayscale):\n",
    "    # preprocess a batch of frames\n",
    "    imgs = np.array([\n",
    "        preprocess_frame(all_frames[idx], grayscale)\n",
    "        for idx in batch_indices\n",
    "    ])\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938aa7f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "globs = [(tp/'box_messy').glob('*.png') for tp in trial_paths]\n",
    "all_frames = [f for g in globs for f in g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c5488d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Split dataset into train and test sets\n",
    "BATCH_SIZE = 32\n",
    "TEST_SET_PROP = 0.1 # 10%\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(all_frames)), test_size=TEST_SET_PROP, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b66795",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_imgs = preprocess_frame_batch(\n",
    "    all_frames, train_indices, grayscale=GS\n",
    ")\n",
    "test_imgs = preprocess_frame_batch(\n",
    "    all_frames, test_indices, grayscale=GS\n",
    ")\n",
    "\n",
    "print(f\"Train set shape: {train_imgs.shape}\")\n",
    "print(f\"Test set shape: {test_imgs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df24405",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from autoencoder.datasets import UnlabeledDataset\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(\n",
    "    UnlabeledDataset(torch.from_numpy(train_imgs)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    UnlabeledDataset(torch.from_numpy(test_imgs)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2dbc3",
   "metadata": {},
   "source": [
    "### **3. Vision autoencoder definition**\n",
    "\n",
    "As we mentioned before, an autoencoder is a type of neural network that learns to compress data into a smaller representation (encoding) and then reconstruct it back (decoding).\n",
    "\n",
    "There is no restriction on the structure of the encoder and the decoder, and they need not be symmetric. \n",
    "\n",
    "However, since we are processing image frames, **convolutional layers** will be helpful becuase they can:\n",
    "- capture spatial features like edges, textures, shapes, etc.\n",
    "- preserve local patterns and share weights across image\n",
    "- detect features efficiently regardless of position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e4d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:34.736874Z",
     "iopub.status.busy": "2025-06-24T18:27:34.736747Z",
     "iopub.status.idle": "2025-06-24T18:27:34.814272Z",
     "shell.execute_reply": "2025-06-24T18:27:34.813534Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from autoencoder.vision_ae import VisualEncoder, VisualDecoder\n",
    "\n",
    "# KERNEL_SIZES = [(4,4), (5,5), (3,3), (3,3)] # kernel sizes for the convolutional layers\n",
    "# KERNEL_STRIDES = [2, 2, 1, 1] # strides\n",
    "# CHANNELS = [8, 8, 16, 16] # number of channels\n",
    "\n",
    "KERNEL_SIZES = [(3,3), (4,4), (4,4), (3,3)] # kernel sizes for the convolutional layers\n",
    "KERNEL_STRIDES = [1, 2, 2, 1] # strides\n",
    "CHANNELS = [8, 8, 16, 16] # number of channels\n",
    "\n",
    "EMB_DIM = 100 # the number of neurons in the latent space (or number of latent features)\n",
    "\n",
    "img_dim_out = IMG_DIM\n",
    "\n",
    "print('Need to make sure all numbers are INTEGERS!\\n')\n",
    "print(f'Input dimension:\\t\\t{1 if GS else 3}x {IMG_DIM}')\n",
    "for i in range(len(KERNEL_SIZES)):\n",
    "    ksize = KERNEL_SIZES[i]\n",
    "    stride = KERNEL_STRIDES[i]\n",
    "\n",
    "    img_dim_out = [\n",
    "        (img_dim_out[i] - ksize[i])/stride + 1\n",
    "        for i in range(len(IMG_DIM))\n",
    "    ]\n",
    "    \n",
    "    print(f'Intermediate dimension {i+1}:\\t{CHANNELS[i]}x {img_dim_out}')\n",
    "\n",
    "print(f'Flattens to:\\t\\t\\t{np.prod(img_dim_out)*CHANNELS[-1]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e1855",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "encoder = VisualEncoder(\n",
    "    visual_embedding_dim = EMB_DIM,\n",
    "    img_dim = IMG_DIM,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(DEVICE)\n",
    "\n",
    "decoder = VisualDecoder(\n",
    "    visual_embedding_dim = EMB_DIM,\n",
    "    img_dim_out = encoder.img_dim_out,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e312c44",
   "metadata": {},
   "source": [
    "Now that we have defined our encoder and decoder, let's see an example of the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf060b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:34.816251Z",
     "iopub.status.busy": "2025-06-24T18:27:34.816136Z",
     "iopub.status.idle": "2025-06-24T18:27:35.098342Z",
     "shell.execute_reply": "2025-06-24T18:27:35.097755Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# visualise examples\n",
    "with torch.no_grad():\n",
    "    for example_batch in dataloader_test:\n",
    "        example_batch = example_batch[np.random.choice(len(example_batch), size=3, replace=False)]\n",
    "        example_batch = example_batch.to(DEVICE)\n",
    "        example_batch_recon = decoder(encoder(example_batch))\n",
    "\n",
    "        fig, axs = plt.subplots(len(example_batch), 2, figsize=(6, 1.5*len(example_batch)))\n",
    "        axs.flat[0].set_title('Original Images')\n",
    "        axs.flat[1].set_title('Reconstructed Images')\n",
    "        for i, (frame_example_img, frame_example_recon) in enumerate(zip(example_batch, example_batch_recon)):\n",
    "            axs[i, 0].imshow(frame_example_img.cpu().numpy().squeeze(), cmap='gray')\n",
    "            axs[i, 1].imshow(frame_example_recon.cpu().numpy().squeeze(), cmap='gray')\n",
    "            axs[i, 0].axis('off')\n",
    "            axs[i, 1].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32823568",
   "metadata": {},
   "source": [
    "This looks like just noise. This is because the autoencoder is untrained. But the good news is: at least we got the dimensions correct!\n",
    "\n",
    "Now, let's train the pair of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c2399",
   "metadata": {},
   "source": [
    "### **4. Autoencoder training**\n",
    "\n",
    "* **Train the model**. This includes a forward pass, computing loss, backpropagation and update weights\n",
    "\n",
    "* If needed, **validate** on the validation set to tune hyperparameters.\n",
    "\n",
    "* **Test** the trained model on unseen data to evaluate performance.\n",
    "\n",
    "* Once the model has reached satisfactory performance, it is ready for **deployment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc27ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:35.575791Z",
     "iopub.status.busy": "2025-06-24T18:27:35.575696Z",
     "iopub.status.idle": "2025-06-24T18:27:35.579096Z",
     "shell.execute_reply": "2025-06-24T18:27:35.578778Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the train and test functions\n",
    "\n",
    "def train_epoch(\n",
    "    enc, dec,\n",
    "    dataloader_train,\n",
    "    loss_fn, optimizer\n",
    "):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch = batch.to(DEVICE)\n",
    "        \n",
    "        # YOUR CODE HERE: forward pass\n",
    "        batch_recon = dec(enc(batch))\n",
    "        loss = loss_fn(batch, batch_recon)\n",
    "        \n",
    "        # YOUR CODE HERE: backward pass and optimisation step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.detach().item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader_train)\n",
    "\n",
    "def test_epoch(\n",
    "    enc, dec,\n",
    "    dataloader_test,\n",
    "    loss_fn,\n",
    "):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_test:\n",
    "            batch = batch.to(DEVICE)\n",
    "            \n",
    "            # YOUR CODE HERE: forward pass\n",
    "            batch_recon = dec(enc(batch))\n",
    "            loss = loss_fn(batch, batch_recon)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bc65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:35.579974Z",
     "iopub.status.busy": "2025-06-24T18:27:35.579879Z",
     "iopub.status.idle": "2025-06-24T18:27:36.132253Z",
     "shell.execute_reply": "2025-06-24T18:27:36.131697Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Putting it all together\n",
    "\n",
    "n_epochs = 100\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    itertools.chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "LR_REDUCTION_FACTOR = 0.1\n",
    "LR_SCHED_PATIENCE = 20\n",
    "LR_SCHED_TH = 1e-3\n",
    "# optional: use a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=LR_REDUCTION_FACTOR,\n",
    "    patience=LR_SCHED_PATIENCE, threshold=LR_SCHED_TH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1d8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T18:27:36.135399Z",
     "iopub.status.busy": "2025-06-24T18:27:36.135193Z",
     "iopub.status.idle": "2025-06-24T19:37:04.764003Z",
     "shell.execute_reply": "2025-06-24T19:37:04.763687Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        encoder, decoder,\n",
    "        dataloader_train,\n",
    "        loss_fn, optimizer\n",
    "    )\n",
    "    test_loss = test_epoch(\n",
    "        encoder, decoder,\n",
    "        dataloader_test,\n",
    "        loss_fn\n",
    "    )\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    # visualize examples\n",
    "    if epoch%10 == 0:\n",
    "        with torch.no_grad():\n",
    "            for example_batch in dataloader_test:\n",
    "                example_batch = example_batch[\n",
    "                    np.random.choice(len(example_batch), size=3, replace=False)\n",
    "                ]\n",
    "                example_batch = example_batch.to(DEVICE)\n",
    "                example_batch_recon = decoder(encoder(example_batch))\n",
    "\n",
    "                fig, axs = plt.subplots(len(example_batch), 2, figsize=(6, 1.5*len(example_batch)))\n",
    "                axs.flat[0].set_title('Original Images')\n",
    "                axs.flat[1].set_title('Reconstructed Images')\n",
    "                for i, (frame_example, frame_recon) in enumerate(zip(example_batch, example_batch_recon)):\n",
    "                    axs[i, 0].imshow(frame_example.cpu().numpy().squeeze(), cmap='gray')\n",
    "                    axs[i, 1].imshow(frame_recon.cpu().numpy().squeeze(), cmap='gray')\n",
    "                    axs[i, 0].axis('off')\n",
    "                    axs[i, 1].axis('off')\n",
    "                plt.show()\n",
    "                break\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f0a7a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4486fb19",
   "metadata": {},
   "source": [
    "### **What have we achieved in this tutorial?**\n",
    "\n",
    "We trained a pair of autoencoders, that can compress images into a latent vector.\n",
    "\n",
    "With a biological constraint (such as non-negative, or sigmoid), then much like the visual cortex, we can interpret the latent vector for each frame as population activity of visual neurons encoding that visual scene.\n",
    "\n",
    "These encodings of the visual scene can now be readily fed into a **Recurrent Neural Network** to generate spatial cell tunings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
