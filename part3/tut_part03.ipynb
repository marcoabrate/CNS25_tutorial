{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0700ad95",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals <br> navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu &emsp;&emsp;&emsp;&emsp;&emsp;&emsp; University College London (UCL)\n",
    "---\n",
    "\n",
    "#### Outline\n",
    "**Part 1: Rat simulation in 3D**\n",
    "- Motion model with `RatInABox`\n",
    "\n",
    "- Environment design\n",
    "\n",
    "- Simulated rat vision with `ratvision`\n",
    "\n",
    "**Part 2: Vision autoencoder**\n",
    "\n",
    "**Part 3: Hippocampus model with RNN**\n",
    "\n",
    "**Part 4: Hidden state representations analysis**\n",
    "- Rate maps\n",
    "\n",
    "- Polar maps\n",
    "\n",
    "- Quantitive metrics\n",
    "\n",
    "- Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c077f",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 3: Hippocampus model with RNN**\n",
    "In this notebook, we will define the **Recurrent Neural Network (RNN)**, which will serve as a model of the hippocampus.\n",
    "\n",
    "In the hippocampal formation, various classes of spatially modulated neurons support navigation by integrating sensory inputs to construct flexible internal models of the world [1,2]. Vision, in particular, plays a crucial role in guiding movement and shaping neural representations of space [3-5].\n",
    "\n",
    "Recent computational approaches have demonstrated considerable success in modelling this ego-to-allocentric transformation using RNNs trained on self-supervised **predictive tasks**. Multiple studies suggest that predictive learning &mdash; anticipating future sensory inputs based on current and past observations &mdash; may be a fundamental computational principle underlying the emergence of spatial representations in both artificial systems and biological brains [6-11].\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- trajectory data from part 1, including speed and rotational speed.\n",
    "- vision data (frames) from part 1.\n",
    "- embedded vision data from the Vision Autoencoder we trained in part 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2d819",
   "metadata": {},
   "source": [
    "### 0. Install and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14611fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27b2ed",
   "metadata": {},
   "source": [
    "### 1. Load trajectories and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a81c33",
   "metadata": {},
   "source": [
    "We load embeddings of vision data recorded from the simulated rat, which we generated in part 2 using the Vision Autoencoder.\n",
    "\n",
    "We load trajectories &mdash; made of x-y positions, head directions, velocities, and rotational velocities.\n",
    "\n",
    "We subsample the data in order to have a temporal gap of one second between samples and increase the complexity of the predictive task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d7ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our trajectories are at 10 FPS\n",
    "# we want to subsample them to 1 FPS\n",
    "STRIDE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = '../data/adult'\n",
    "trial_paths = sorted([p for p in Path(d).iterdir() if 'exp' in p.name])\n",
    "trial_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37264c",
   "metadata": {},
   "source": [
    "Import the function to subsample the data. Velocities must be integrated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223262f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_multiple_subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb10016",
   "metadata": {},
   "source": [
    "Load trajectories and embeddings which will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_embeddings = []\n",
    "train_vel, train_rotvel, train_pos, train_hds = [], [], [], []\n",
    "\n",
    "for idx in range(20):\n",
    "    tp = trial_paths[idx]\n",
    "    train_embeddings.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'vision_embeddings.npy'), stride=STRIDE)\n",
    "    )\n",
    "    train_pos.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'positions.npy'), stride=STRIDE)\n",
    "    )\n",
    "    train_hds.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'thetas.npy')[..., None], stride=STRIDE)\n",
    "    )\n",
    "    train_vel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'velocities.npy'), stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    train_rotvel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'rot_velocities.npy')[..., None],\n",
    "            stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
    "train_pos = np.concatenate(train_pos, axis=0)\n",
    "train_hds = np.concatenate(train_hds, axis=0)\n",
    "train_vel = np.concatenate(train_vel, axis=0)\n",
    "train_rotvel = np.concatenate(train_rotvel, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd805c2c",
   "metadata": {},
   "source": [
    "Load trajectories and embeddings which will be used to test the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "test_vel, test_rotvel, test_pos, test_hds = [], [], [], []\n",
    "\n",
    "for idx in range(20, 23):\n",
    "    tp = trial_paths[idx]\n",
    "    test_embeddings.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'vision_embeddings.npy'), stride=STRIDE)\n",
    "    )\n",
    "    test_pos.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'positions.npy'), stride=STRIDE)\n",
    "    )\n",
    "    test_hds.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'thetas.npy')[..., None], stride=STRIDE)\n",
    "    )\n",
    "    test_vel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'velocities.npy'), stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    test_rotvel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'rot_velocities.npy')[..., None],\n",
    "            stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "test_embeddings = np.concatenate(test_embeddings, axis=0)\n",
    "test_pos = np.concatenate(test_pos, axis=0)\n",
    "test_hds = np.concatenate(test_hds, axis=0)\n",
    "test_vel = np.concatenate(test_vel, axis=0)\n",
    "test_rotvel = np.concatenate(test_rotvel, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783048a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Train velocities shape: {train_vel.shape}\")\n",
    "print(f\"Train rotational velocities shape: {train_rotvel.shape}\")\n",
    "print(f\"Train positions shape: {train_pos.shape}\")\n",
    "print(f\"Train head directions shape: {train_hds.shape}\")\n",
    "print()\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Test velocities shape: {test_vel.shape}\")\n",
    "print(f\"Test rotational velocities shape: {test_rotvel.shape}\")\n",
    "print(f\"Test positions shape: {test_pos.shape}\")\n",
    "print(f\"Test head directions shape: {test_hds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2374e24",
   "metadata": {},
   "source": [
    "### 2. Define dataloader\n",
    "\n",
    "To define the next-step prediction task, we use a **Dataset** and **DataLoader** to make our life easier. The **Dataset** prepares sensory embeddings as well as motion signals to batches of paired inputs and labels, while the **DataLoader** sequentially generates batches of data during training and testing.\n",
    "\n",
    "The Dataset and Dataloader classes inherit PyTorch's built-in `torch.utils.data.Dataset` and `torch.utils.data.DataLoader` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ecaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embs, vels, rot_vels, pos, hds, tsteps=9):\n",
    "        '''\n",
    "        The initialisation function for the SensoryDataset class.\n",
    "        At initialisation, arrays are converted to tensors.\n",
    "        N is the number of trials, T is the number of time steps, and D is the number of features.\n",
    "\n",
    "        Args:\n",
    "            embs: The sensory embeddings of shape (N, T, D)\n",
    "            vels: The velocity of shape (N, T-1, 2)\n",
    "            rot_vels: The rotational velocities of shape (N, T-1, 1)\n",
    "            pos: The x-y positions of shape (N, T, 2)\n",
    "            hds: The heading directions of shape (N, T, 1)\n",
    "            tsteps: The number of time steps for each batch.\n",
    "                By default, this is set to 9 (seconds if frequency is 1 Hz) \n",
    "        '''\n",
    "        self.embs = torch.from_numpy(embs)\n",
    "        self.vels = torch.from_numpy(vels)\n",
    "        self.rot_vels = torch.from_numpy(rot_vels)\n",
    "        self.pos = torch.from_numpy(pos)\n",
    "        self.hds = torch.from_numpy(hds)\n",
    "        \n",
    "        self.tsteps = tsteps\n",
    "    \n",
    "    def __len__(self):\n",
    "        # YOUR CODE HERE (1)\n",
    "        #  how many samples are in the dataset?\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Returns a batch of sensory embeddings, motion signals,\n",
    "        trajectory data, future sensory embeddings.\n",
    "        '''\n",
    "        vels, rot_vels, pos, hds, embs_labels = [], [], [], [], []\n",
    "\n",
    "        # YOUR CODE HERE (2)\n",
    "        # get a sequence of data \n",
    "        start_idx, end_idx = None, None\n",
    "\n",
    "        embs = self.embs[:, start_idx:end_idx]\n",
    "        vels = self.vels[:, start_idx:end_idx]\n",
    "        rot_vels = self.rot_vels[:, start_idx:end_idx]\n",
    "        pos = self.pos[:, start_idx:end_idx]\n",
    "        hds = self.hds[:, start_idx:end_idx]\n",
    "\n",
    "        embs_labels = self.embs[:, start_idx+1 : end_idx+1]\n",
    "        \n",
    "        return embs, vels, rot_vels, pos, hds, embs_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8708a59",
   "metadata": {},
   "source": [
    "It's important to note that we don't shuffle the datasets because sensory data and hidden states are dependent on the previous batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af041e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    SensoryDataset(\n",
    "        train_embeddings, train_vel, train_rotvel, train_pos, train_hds\n",
    "    ), shuffle=False\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    SensoryDataset(\n",
    "        test_embeddings, test_vel, test_rotvel, test_pos, test_hds\n",
    "    ), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830544ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in train_dataloader:\n",
    "    embs, vels, rot_vels, pos, hds, embs_labels = b\n",
    "    print('Shape of the batch (1, BATCH_SIZE, TIMESTEPS, N_FEATURES)')\n",
    "    print()\n",
    "    print(f'Embeddings:\\t\\t{embs.shape}')\n",
    "    print(f'Velocities:\\t\\t{vels.shape}')\n",
    "    print(f'Rot. velocities:\\t{rot_vels.shape}')\n",
    "    print(f'Positions:\\t\\t{pos.shape}')\n",
    "    print(f'Head directions:\\t{hds.shape}')\n",
    "    print(f'Embeddings labels:\\t{embs_labels.shape}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e237d0",
   "metadata": {},
   "source": [
    "**Note:** we switched our terminology from \"vision\" embeddings to the more general \"sensory\" embeddings because in principle here one can model all kind of sensory inputs &mdash; such as auditory, olfactory, and tactile modalities &mdash; which might be integrated in the hippocampus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ff46f",
   "metadata": {},
   "source": [
    "### 3. Define the RNN\n",
    "\n",
    "Let's define the RNN that will serve as our hippocampus. This network will use a customized `RNNModule` class that applies a Sigmoid activation function to the hidden state &mdash; forcing a biologically-plausible constraint of non-negativity. The hidden states are projected to predict the next sensory state via a linear layer.\n",
    "\n",
    "The output of the Sigmoid lies between 0 and 1, and can be interpreted as the (scaled) firing rate (or simply activity) of the neurons in the hidden state.\n",
    "\n",
    "<img src=\"rnn_arch.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "$Y_t$: sensory embeddings &emsp; $\\text{v}_t$: velocity &emsp; $\\omega_t$: rotational velocity\n",
    "\n",
    "$W_x$: `in2hidden` &emsp; $W_h$: `hidden2hidden` &emsp; $W_o$: `hidden2outputs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87905549",
   "metadata": {},
   "source": [
    "We define the single RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2940e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (3)\n",
    "# Define the RNN cell class\n",
    "class RNNCell(torch.nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b956d60",
   "metadata": {},
   "source": [
    "We define the RNN module, which accepts sequential data and recurrently calls the RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, device, n_inputs, n_hidden, bias\n",
    "    ):\n",
    "        super(RNNModule, self).__init__()\n",
    "\n",
    "        self.rnn_cell = RNNCell(n_inputs, n_hidden, bias)\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        '''\n",
    "        inputs is a sequence of shape (BATCH_SIZE, TIMESTEPS, N_FEATURES)\n",
    "        hidden is the hidden state from the previous batch (if present), of shape (BATCH_SIZE, N_HIDDEN)\n",
    "\n",
    "        hidden_new is a sequence of shape (BATCH_SIZE, TIMESTEPS, N_HIDDEN)\n",
    "        '''\n",
    "        hidden_new = torch.zeros(inputs.shape[0], inputs.shape[1], self.n_hidden).to(self.device)\n",
    "\n",
    "        if hidden is None:\n",
    "            # initialize hidden state to zero if not provided\n",
    "            h_out = torch.zeros(inputs.shape[0], self.n_hidden).to(self.device)\n",
    "        else:\n",
    "            h_out = hidden\n",
    "\n",
    "        window_size = inputs.shape[1]\n",
    "\n",
    "        # YOUR CODE HERE (4)\n",
    "        # loop over the sequence \"inputs\"\n",
    "        pass\n",
    "\n",
    "        # return all hidden states\n",
    "        return hidden_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e64c1",
   "metadata": {},
   "source": [
    "Put everything together and define $W_o$ (`hidden2outputs`) for next-step prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictiveRNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        device, n_inputs, n_hidden, n_outputs, bias=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = RNNModule(\n",
    "            device, n_inputs, n_hidden, bias=bias\n",
    "        )\n",
    "\n",
    "        # YOUR CODE HERE (5)\n",
    "        # define the output layer\n",
    "        self.hidden2outputs = None\n",
    "\n",
    "    def inputs2hidden(self, inputs, hidden):\n",
    "        # just makes sure to pass the right shape of\n",
    "        # the hidden state, if given\n",
    "        if hidden is not None:\n",
    "            return self.rnn(inputs, hidden[None, ...])\n",
    "        else:\n",
    "            return self.rnn(inputs)\n",
    "    \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        '''\n",
    "        inputs is a sequence of shape (BATCH_SIZE, TIMESTEPS, N_FEATURES)\n",
    "        hidden is the hidden state from the previous batch (if present), of shape (BATCH_SIZE, N_HIDDEN)\n",
    "\n",
    "        hidden_new is a sequence of shape (BATCH_SIZE, TIMESTEPS, N_HIDDEN)\n",
    "\n",
    "        outputs is a sequence of shape (BATCH_SIZE, TIMESTEPS, N_OUTPUTS)\n",
    "        '''\n",
    "        hidden_new = self.inputs2hidden(inputs, hidden)\n",
    "\n",
    "        outputs = self.hidden2outputs(hidden_new)\n",
    "\n",
    "        return outputs, hidden_new[:,-1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229dde1",
   "metadata": {},
   "source": [
    "Note that the input dimension is the sum of the visual embedding dimension and the motion signals, while the output dimension is only the visual embedding dimension.\n",
    "\n",
    "Here we also define the number of neurons in the hidden state, which we set to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of neurons in the hidden state\n",
    "N_HIDDEN = 500\n",
    "\n",
    "visual_embedding_dim = train_embeddings.shape[-1]\n",
    "motion_signal_dim = train_vel.shape[-1] + train_rotvel.shape[-1]\n",
    "\n",
    "rnn = PredictiveRNN(\n",
    "    DEVICE,\n",
    "    n_inputs=visual_embedding_dim + motion_signal_dim,\n",
    "    n_hidden=N_HIDDEN,\n",
    "    n_outputs=visual_embedding_dim\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da5fe6",
   "metadata": {},
   "source": [
    "### 4. Define training loops\n",
    "\n",
    "Training the RNN is similar to training the autoencoder, except that we need to **pass the hidden state from one batch to the next**, it's important to note that batches are now sequential rather than independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbfa5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    rnn,\n",
    "    dataloader,\n",
    "    loss_fn, optimizer\n",
    "):\n",
    "    rnn.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "\n",
    "    # Initialize hidden state to None\n",
    "    hidden_state = None\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embs, vels, rot_vels, _, _, embs_labels = batch\n",
    "        \n",
    "        # YOUR CODE HERE (6)\n",
    "        # concatenate the embeddings, velocities, and rotational velocities\n",
    "        # and pass the inputs through the RNN\n",
    "        inputs = None\n",
    "\n",
    "        outputs, hidden_new = rnn(inputs, hidden_state)\n",
    "\n",
    "        embs_labels = embs_labels.squeeze(dim=0).to(DEVICE)\n",
    "\n",
    "        # YOUR CODE HERE (7)\n",
    "        # compute the loss and its gradients\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # (optional) clip the gradients\n",
    "        torch.nn.utils.clip_grad_norm_(rnn.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Assign new RNN hidden state to variable.\n",
    "        # Detach it to prevent backpropagation\n",
    "        # through the entire history\n",
    "        hidden_state = hidden_new.detach()\n",
    "\n",
    "        batch_losses.append(loss.detach().item())\n",
    "\n",
    "    return batch_losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8a3d2",
   "metadata": {},
   "source": [
    "A paired function to test and evaluate the RNN has been provided for convenience. This function additionally saves the hidden states at each step. This is needed for computing the rate maps in the next part of this tutorial. It is structured like this:\n",
    "\n",
    "```python\n",
    "def evaluate_rnn(device, rnn, dataloader, loss_fn, for_ratemaps):\n",
    "    ...\n",
    "    return dictionary\n",
    "```\n",
    "\n",
    "Where, `dictionary` contains the following keys when `for_ratemaps=True`:\n",
    "* `batch_losses`: The loss for each batch.\n",
    "\n",
    "* `hidden_states`: The hidden states at each timestep.\n",
    "\n",
    "* `positions`: The x-y positions at each timestep.\n",
    "\n",
    "* `head_directions`: The head direction at each timestep.\n",
    "\n",
    "* `outputs`: The predicted output visual embeddings at each timestep.\n",
    "\n",
    "* `embs_labels`: The expected output visual embeddings at each timestep (ground truth).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab6d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae62ad",
   "metadata": {},
   "source": [
    "### 5. Train RNN on self-supervised predictive task\n",
    "\n",
    "We will define some parameters as we did for the autoencoder and train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100 # 1_000 to perform a full training\n",
    "learning_rate = 5e-4\n",
    "\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# optional: use a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.5,\n",
    "    patience=50, threshold=1e-3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104cc1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "train_means, test_means = [], []\n",
    "train_stds, test_stds = [], []\n",
    "test_io_dist = []\n",
    "epochs = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_losses = None\n",
    "    test_dict = None\n",
    "    \n",
    "    train_losses = train_epoch(rnn, train_dataloader, loss_fn, optimizer)\n",
    "    d = evaluate_rnn(DEVICE, rnn, test_dataloader, loss_fn, for_ratemaps=False)\n",
    "    \n",
    "    train_mean = np.mean(train_losses)\n",
    "\n",
    "    test_losses = d['batch_losses']\n",
    "    test_mean = np.mean(test_losses)\n",
    "\n",
    "    scheduler.step(test_mean)\n",
    "\n",
    "    train_means.append(train_mean)\n",
    "    test_means.append(test_mean)\n",
    "\n",
    "    test_io_dist.append(np.mean(d['batch_io_dists']))\n",
    "\n",
    "    # Clear previous plot output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Create new plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(range(epoch+1), train_means, label='Train loss', color='blue')\n",
    "    plt.plot(range(epoch+1), test_means, label='Test loss', color='orange')\n",
    "    plt.plot(range(epoch+1), test_io_dist, label='I/O dist', color='gray', linestyle=':')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"L1\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a27eda",
   "metadata": {},
   "source": [
    "Train and test loss for a ful training process.\n",
    "\n",
    "![Train and test loss](train_test_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531283d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-comment this to save your new model's weights\n",
    "\n",
    "# torch.save(rnn.state_dict(), 'rnn.pth')\n",
    "\n",
    "# load as\n",
    "# rnn = PredictiveRNN(\n",
    "#     DEVICE,\n",
    "#     n_inputs=visual_embedding_dim + motion_signal_dim,\n",
    "#     n_hidden=500,\n",
    "#     n_outputs=visual_embedding_dim\n",
    "# )\n",
    "# rnn.load_state_dict(torch.load('rnn.pth', weights_only=True)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a234f",
   "metadata": {},
   "source": [
    "### **Summary: what have we achieved?**\n",
    "\n",
    "We defined and trained a custom **Recurrent Neural Network (RNN)**, presented as a model of the hippocampus.\n",
    "\n",
    "We leveraged the **Dataset** and **DataLoader** Pytorch classes to define the self-supervised predictive task and generate sequential batches of data.\n",
    "\n",
    "In the next part, we will explore the spatial representations of the neurons in the hidden state of the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f54c1",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] J. O'keefe and L. Nadel. The hippocampus as a cognitive map. Oxford University Press, 1978.\n",
    "\n",
    "[2] T. E. Behrens, T. H. Muller, J. C. Whittington, S. Mark, A. B. Baram, K. L. Stachenfeld, Z. Kurth-Nelson. What is a cognitive map? Organizing knowledge for flexible behavior. Neuron, 100(2):490-509, 2024.\n",
    "\n",
    "[3] L. Acharya, Z. M. Aghajan, C. Vuong, J. J. Moore, and M. R. Mehta. Causal influence of visual cues on hippocampal directional selectivity. Cell, 164(1):197–207, 2016.\n",
    "\n",
    "[4] K. J. Jeffery and J. M. O’Keefe. Learned interaction of visual and idiothetic cues in the control of place field orientation. Experimental brain research, 127:151–161, 1999.\n",
    "\n",
    "[5] J. O'Keefe and D. H. Conway. Hippocampal place units in the freely moving rat: why they fire where they fire. Experimental brain research, 31:573–590, 1978.\n",
    "\n",
    "[6] C. J. Cueva and X.-X. Wei. Emergence of grid-like representations by training recurrent neural networks to perform spatial localization. arXiv 1803.07770, 2018.\n",
    "\n",
    "[7] J. Gornet and M. Thomson. Automated construction of cognitive maps with visual predictive coding. Nature Machine Intelligence, 6(7):820–833, 2024.\n",
    "\n",
    "[8] D. Levenstein, A. Efremov, R. H. Eyono, A. Peyrache, and B. Richards. Sequential predictive learning is a unifying theory for hippocampal representation and replay. bioRxiv 2024.04.28.591528, 2024.\n",
    "\n",
    "[9] S. Recanatesi, M. Farrell, G. Lajoie, S. Deneve, M. Rigotti, and E. Shea-Brown. Predictive learning as a network mechanism for extracting low-dimensional latent space representations. Nature communications, 12(1):1417, 2021.\n",
    "\n",
    "[10] B. Uria, B. Ibarz, A. Banino, V. Zambaldi, D. Kumaran, D. Hassabis, C. Barry, and C. Blundell. The spatial memory pipeline: a model of egocentric to allocentric understanding in mammalian brains. BioRxiv 2020.11.11.378141, 2022.\n",
    "\n",
    "[11] J. C. Whittington, T. H. Muller, S. Mark, G. Chen, C. Barry, N. Burgess, and T. E. Behrens. The tolman eichenbaum-machine: unifying space and relational memory through generalization in the hippocampal formation. Cell, 183(5):1249–1263, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07819ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
