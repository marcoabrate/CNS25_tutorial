{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19266574",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu\n",
    "---\n",
    "\n",
    "##### Outline\n",
    "Rat simulation:\n",
    "- Motion model (RatInABox)\n",
    "- Environment design (Blender)\n",
    "- Simulated rat vision (ratvision)\n",
    "\n",
    "Vision autoencoder\n",
    "\n",
    "Hippocampus model (RNN):\n",
    "- RNN definition\n",
    "- Data loading\n",
    "- Training\n",
    "\n",
    "Hidden state representations analysis:\n",
    "- Rate maps\n",
    "- Polar maps\n",
    "- Quantitive metrics\n",
    "- Comparison with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0d87d",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 2: Training a Vision Autoencoder**\n",
    "\n",
    "In this notebook, we will write code to train an **Autoencoder**. An autoencoder is a pair of artificial neural networks that compresses information into a low-dimensional embedding through the first module (aka encoder) and reconstructs it to its original form through the second module (aka decoder).\n",
    "\n",
    "Neuroscientists use vision autoencoders to model how neurons might represent visual stimuli in the brain. The visual cortex receives complex images and it is able to extract key features - such as edges, motions and shapes - into more compact forms (low-dimensional embedding). This non-linear dimensionality reduction process, along with the reconstruction of the original image, can be compared to an autoencoder.\n",
    "\n",
    "We will use the **PyTorch** package for this tutorial.\n",
    "\n",
    "Before starting this notebook, make sure you have video recordings from the previous part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05ff81b",
   "metadata": {},
   "source": [
    "### **0. Import and install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee49780",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8eff9",
   "metadata": {},
   "source": [
    "### **1. Visualize example frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bea2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_paths = [p for p in Path('/Users/marco/Downloads/vrtopc/box/run').iterdir() if 'exp' in p.name]\n",
    "\n",
    "trial_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c66925",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_example = trial_paths[0] / 'box_messy' / 'frame0001.png'\n",
    "plt.imshow(np.array(Image.open(frame_example)), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d45835",
   "metadata": {},
   "source": [
    "### **2 Writing an autoencoder**\n",
    "\n",
    "We begin by coding up our autoencoders.\n",
    "\n",
    "As we mentioned before, an autoencoder is a type of neural network that learns to compress data into a smaller representation (encoding) and then reconstruct it back (decoding).\n",
    "\n",
    "There is no restriction on the structure of the encoder and the decoder, and they need not be symmetric. \n",
    "\n",
    "However, since we are processing image frames, **convolutional layers** will be helpful becuase they can:\n",
    "* capture spatial features like edges, textures, shapes, etc.\n",
    "\n",
    "* preserve local patterns and share weights across image\n",
    "\n",
    "* detect features efficiently regardless of position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c58e4a0",
   "metadata": {},
   "source": [
    "First, let's defome some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafc4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VisualEncoder(nn.Module):\n",
    "#     def __init__(self, visual_embedding_dim, img_dim, grayscale):\n",
    "#         super(VisualEncoder, self).__init__()\n",
    "\n",
    "#         self.c_channel = 1 if grayscale else 3\n",
    "        \n",
    "#         self.encoder_layers = nn.Sequential(\n",
    "#             # WRITE YOUR CODE HERE\n",
    "            \n",
    "#             nn.Conv2d(self.c_channel, 16, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#         )\n",
    "        \n",
    "#         # now we flatten and project linearly to the target embedding dimension.\n",
    "#         self.threshold_dims = self.get_threshold_dims(img_dim)\n",
    "\n",
    "#         self.linear_projection = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(np.prod(self.threshold_dims), visual_embedding_dim),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "    \n",
    "#     def get_threshold_dims(self, img_dim):\n",
    "#         # a helper function to obtain the CNN-processed dimension.\n",
    "#         h, w = img_dim\n",
    "#         with torch.no_grad():\n",
    "#             return self.encoder_layers.cpu()(torch.rand(1, self.c_channel, h, w)).shape[1:]\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         img_conv = self.encoder_layers(img)\n",
    "#         emb = self.linear_projection(img_conv)\n",
    "#         return emb\n",
    "\n",
    "\n",
    "# # define a decoder\n",
    "# class VisualDecoder(nn.Module):\n",
    "#     def __init__(self, visual_embedding_dim, threshold_dims, img_dim, grayscale):\n",
    "#         super(VisualDecoder, self).__init__()\n",
    "\n",
    "#         self.c_channel = 1 if grayscale else 3\n",
    "#         self.threshold_dims = threshold_dims\n",
    "#         self.img_dim = img_dim\n",
    "\n",
    "#         self.linear_projection = nn.Sequential(\n",
    "#             nn.Linear(visual_embedding_dim, np.prod(threshold_dims)),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        \n",
    "#         self.decoder_layers = nn.Sequential(\n",
    "#             # WRITE YOUR CODE HERE\n",
    "            \n",
    "#             # nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "#             # nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(16, self.c_channel, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.Sigmoid() # projects to (0, 1) scale\n",
    "#         )\n",
    "\n",
    "#     def forward(self, emb):\n",
    "#         emb = self.linear_projection(emb)\n",
    "#         emb = emb.reshape(emb.shape[0], self.threshold_dims[0], self.threshold_dims[1], self.threshold_dims[2])\n",
    "        \n",
    "#         img_dec = self.decoder_layers(emb)\n",
    "#         return img_dec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        visual_embedding_dim, img_dim, grayscale,\n",
    "        kernel_sizes: list[tuple[int]] = [(4,5)],\n",
    "        kernel_strides: list[int] = [3],\n",
    "        channels: list[int] = [8],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        inc = 1 if grayscale else 3\n",
    "        img_dim_out = img_dim\n",
    "\n",
    "        encoder_conv_layers = []\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            ksize = kernel_sizes[i]\n",
    "            stride = kernel_strides[i]\n",
    "            encoder_conv_layers.append(torch.nn.Conv2d(\n",
    "                in_channels = (inc if i == 0 else channels[i-1]),\n",
    "                out_channels = channels[i],\n",
    "                kernel_size = ksize,\n",
    "                stride = stride,\n",
    "            ))\n",
    "\n",
    "            img_dim_out = [\n",
    "                int((img_dim_out[i] - ksize[i])/stride + 1)\n",
    "                for i in range(len(img_dim))\n",
    "            ]\n",
    "            \n",
    "            encoder_conv_layers.append(torch.nn.ReLU())\n",
    "\n",
    "        self.encoder_conv = torch.nn.Sequential(*encoder_conv_layers)\n",
    "        \n",
    "        self.img_dim_out = img_dim_out\n",
    "        \n",
    "        print(f'Encoder convolution layer: {self.encoder_conv}')\n",
    "        print(f'Final convolution size: {channels[-1]}x{img_dim_out}')\n",
    "        print(f'Flattens to {channels[-1]*np.prod(img_dim_out)}\\n')\n",
    "            \n",
    "        self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "\n",
    "        self.encoder_lin = torch.nn.Sequential(\n",
    "            torch.nn.Linear(channels[-1]*np.prod(img_dim_out), visual_embedding_dim),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        print(f'Encoder linear layer: {self.encoder_lin}')\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        img_conv = self.encoder_conv(img)\n",
    "        img_conv_flatten = self.flatten(img_conv)\n",
    "\n",
    "        embeddings = self.encoder_lin(img_conv_flatten)\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e4cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualDecoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        visual_embedding_dim, img_dim_out, grayscale,\n",
    "        kernel_sizes: list[tuple[int]] = [(4,5)],\n",
    "        kernel_strides: list[int] = [3],\n",
    "        channels: list[int] = [8],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        inc = 1 if grayscale else 3\n",
    "\n",
    "        self.decoder_lin = torch.nn.Linear(visual_embedding_dim, channels[-1]*np.prod(img_dim_out))\n",
    "        print(f'\\nDecoder linear layer: {self.decoder_lin}\\n')\n",
    "\n",
    "        self.unflatten = torch.nn.Unflatten(\n",
    "            dim=1,\n",
    "            unflattened_size=channels[-1:]+img_dim_out\n",
    "        )\n",
    "            \n",
    "        decoder_conv_layers = []\n",
    "        for i in range(len(kernel_sizes)-1, -1, -1):\n",
    "            decoder_conv_layers.append(torch.nn.ConvTranspose2d(\n",
    "                in_channels = channels[i],\n",
    "                out_channels = (inc if i == 0 else channels[i-1]),\n",
    "                kernel_size = kernel_sizes[i],\n",
    "                stride = kernel_strides[i],\n",
    "            ))\n",
    "            decoder_conv_layers.append(\n",
    "                torch.nn.Sigmoid() if i == 0 else torch.nn.ReLU()\n",
    "            )\n",
    "\n",
    "        self.decoder_conv = torch.nn.Sequential(*decoder_conv_layers)\n",
    "        print(f'Decoder convolution layer: {self.decoder_conv}')\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        img_conv = self.decoder_lin(embeddings)\n",
    "\n",
    "        img_conv = self.unflatten(img_conv)\n",
    "        \n",
    "        img_reconstructed = self.decoder_conv(img_conv)\n",
    "        \n",
    "        return img_reconstructed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779071b",
   "metadata": {},
   "source": [
    "Now that we have defined our encoder and decoder, let's initialise them and see an example of the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f104c18",
   "metadata": {},
   "source": [
    "We will import two pre-defined functions, ```preprocess_frame()``` and ```preprocess_frame_batch()``` from ```utils.py```.\n",
    "\n",
    "The ```preprocess_frame``` accepts the path of a frame, then converts it to RGB values and normalise it 1 so that we have a ```(width, length, channels)``` array of numbers between 0 and 1.\n",
    "\n",
    "Optionally, the frame can be converted to grayscale, so that the channel dimension is 1. We can also decide to add Gaussian noise to the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, grayscale, img_noise_std):\n",
    "    img = Image.open(frame)\n",
    "    if grayscale: \n",
    "        img = img.convert('L')\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "\n",
    "    # add noise to the image if img_noise_std > 0\n",
    "    if img_noise_std > 0:\n",
    "        img += np.random.normal(0, img_noise_std, size=img.shape)\n",
    "\n",
    "    # normalise RGB to (0, 1) scale\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    if grayscale:\n",
    "        img = img[None, ...] # (1, h, w) if grayscale\n",
    "    else:\n",
    "        img = np.moveaxis(img, -1, 0) # reshape to (3, h, w) if RGB\n",
    "\n",
    "    return img\n",
    "\n",
    "def preprocess_frame_batch(all_frames, batch_indices, grayscale, img_noise_std):\n",
    "    # preprocess a batch of frames\n",
    "    imgs = np.array([\n",
    "        preprocess_frame(all_frames[idx], grayscale, img_noise_std)\n",
    "        for idx in batch_indices\n",
    "    ])\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0e4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 50 # the number of neurons in the latent space (or number of latent features)\n",
    "IMG_DIM = (64, 128) # (height, width) of the input images\n",
    "GS = True # whether to use grayscale images\n",
    "IMG_NOISE_STD = 0 # standard deviation of the noise to be added to the images\n",
    "\n",
    "KERNEL_SIZES = [(2, 2), (3, 3)] # kernel sizes for the convolutional layers\n",
    "KERNEL_STRIDES = [1, 2] # strides\n",
    "CHANNELS = [16, 32] # number of channels\n",
    "\n",
    "encoder = VisualEncoder(\n",
    "    visual_embedding_dim = EMB_DIM,\n",
    "    img_dim = IMG_DIM,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(device)\n",
    "\n",
    "decoder = VisualDecoder(\n",
    "    visual_embedding_dim = EMB_DIM,\n",
    "    img_dim_out = encoder.img_dim_out,\n",
    "    grayscale = GS,\n",
    "    kernel_sizes = KERNEL_SIZES,\n",
    "    kernel_strides = KERNEL_STRIDES,\n",
    "    channels = CHANNELS\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise an example\n",
    "with torch.no_grad():\n",
    "    frame_example_img = preprocess_frame(frame_example, grayscale=GS, img_noise_std=IMG_NOISE_STD)[None, ...]\n",
    "    frame_example_recon = decoder(encoder(\n",
    "        torch.from_numpy(frame_example_img).to(device)\n",
    "    ))\n",
    "    \n",
    "frame_example_recon = frame_example_recon.cpu().numpy()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].set_title('Original Frame')\n",
    "axs[0].imshow(frame_example_img[0, 0], cmap='gray')\n",
    "axs[1].set_title('Reconstructed Frame')\n",
    "axs[1].imshow(frame_example_recon[0, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32823568",
   "metadata": {},
   "source": [
    "This looks like just noise. This is because the autoencoder is untrained. But the good news is: at least we got the dimensions correct!\n",
    "\n",
    "Now, let's train the pair of networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917c2399",
   "metadata": {},
   "source": [
    "### **3 Training an autoencoder**\n",
    "\n",
    "The process of training a neural network is well established. In general, we follow these steps:\n",
    "\n",
    "* **Collect and pre-process data**: in the case of image frames, we need to consider normalising the RGB values.\n",
    "\n",
    "* **Split the dataset** into train, test (and optionally validation) sets\n",
    "\n",
    "* **Initialise model weights**, which we have just done!\n",
    "\n",
    "* **Train the model**. This includes a forward pass, computing loss, backpropagation and update weights\n",
    "\n",
    "* Where needed, **validate** on the validation set to tune hyperparameters.\n",
    "\n",
    "* **Test** the trained model on unseen data to evaluate performance.\n",
    "\n",
    "* Once the model has reached satisfactory performance, it is ready for **deployment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "globs = [(tp/'box_messy').glob('*.png') for tp in trial_paths]\n",
    "all_frames = [f for g in globs for f in g][:20_480]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "BATCH_SIZE = 2048\n",
    "TEST_SET_PROP = 0.1 # 10%\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    np.arange(len(all_frames)), test_size=TEST_SET_PROP, shuffle=True, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66724438",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = np.split(\n",
    "    train_indices[:(len(train_indices)//BATCH_SIZE*BATCH_SIZE)],\n",
    "    len(train_indices)//BATCH_SIZE\n",
    ")\n",
    "test_batches = np.split(\n",
    "    test_indices[:(len(test_indices)//BATCH_SIZE*BATCH_SIZE)],\n",
    "    len(test_indices)//BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc27ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test functions\n",
    "\n",
    "def train_epoch(\n",
    "    enc, dec,\n",
    "    all_frames, train_batches,\n",
    "    loss_fn, optimizer, scheduler=None\n",
    "):\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_indices in train_batches:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        imgs = preprocess_frame_batch(\n",
    "            all_frames, batch_indices, grayscale=GS, img_noise_std=IMG_NOISE_STD\n",
    "        )\n",
    "        imgs = torch.from_numpy(imgs).to(device)\n",
    "        \n",
    "        # YOUR CODE HERE: forward pass\n",
    "        imgs_recon = dec(enc(imgs))\n",
    "        loss = loss_fn(imgs, imgs_recon)\n",
    "        \n",
    "        # YOUR CODE HERE: backward pass and optimisation step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.detach().item()\n",
    "            \n",
    "    return epoch_loss / len(train_batches)\n",
    "\n",
    "def test_epoch(\n",
    "    enc, dec,\n",
    "    all_frames, test_batches,\n",
    "    loss_fn,\n",
    "):\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_indices in test_batches:\n",
    "            imgs = preprocess_frame_batch(\n",
    "                all_frames, batch_indices, grayscale=GS, img_noise_std=IMG_NOISE_STD\n",
    "            )\n",
    "            imgs = torch.from_numpy(imgs).to(device)\n",
    "            \n",
    "            # YOUR CODE HERE: forward pass\n",
    "            imgs_recon = dec(enc(imgs))\n",
    "            loss = loss_fn(imgs, imgs_recon)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "\n",
    "n_epochs = 10\n",
    "loss_fn = nn.L1Loss()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    itertools.chain(encoder.parameters(), decoder.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# optional: use a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a1d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(\n",
    "        encoder, decoder,\n",
    "        all_frames, train_batches,\n",
    "        loss_fn, optimizer, scheduler\n",
    "    )\n",
    "    test_loss = test_epoch(\n",
    "        encoder, decoder,\n",
    "        all_frames, test_batches,\n",
    "        loss_fn\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        frame_example_recon = decoder(encoder(\n",
    "            torch.from_numpy(frame_example_img).to(device)\n",
    "        ))\n",
    "    frame_example_recon = frame_example_recon.cpu().numpy()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].set_title('Original Frame')\n",
    "    axs[0].imshow(frame_example_img[0, 0], cmap='gray')\n",
    "    axs[1].set_title('Reconstructed Frame')\n",
    "    axs[1].imshow(frame_example_recon[0, 0], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486fb19",
   "metadata": {},
   "source": [
    "### **What have we achieved in this tutorial?**\n",
    "\n",
    "We trained a pair of autoencoders, that can compress images into a latent vector.\n",
    "\n",
    "With a biological constraint (such as non-negative, or sigmoid), then much like the visual cortex, we can interpret the latent vector for each frame as population activity of visual neurons encoding that visual scene.\n",
    "\n",
    "These encodings of the visual scene can now be readily fed into a **Recurrent Neural Network** to generate spatial cell tunings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
