{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0700ad95",
   "metadata": {},
   "source": [
    "---\n",
    "# Modelling hippocampal neurons of animals navigating in VR with recurrent neural networks\n",
    "### Marco P. Abrate, Daniel Liu\n",
    "---\n",
    "\n",
    "##### Outline\n",
    "Rat simulation:\n",
    "- Motion model (RatInABox)\n",
    "- Environment design (Blender)\n",
    "- Simulated rat vision (ratvision)\n",
    "\n",
    "Vision autoencoder\n",
    "\n",
    "Hippocampus model (RNN):\n",
    "- RNN definition\n",
    "- Data loading\n",
    "- Training\n",
    "\n",
    "Hidden state representations analysis:\n",
    "- Rate maps\n",
    "- Polar maps\n",
    "- Quantitive metrics\n",
    "- Comparison with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c077f",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 3: Modelling the hippocampus with an RNN**\n",
    "In this notebook, we will write code to define the Recurrent Neural Network (**RNN**), which will serve as the model of the hippocampus.\n",
    "\n",
    "RNNs are models designed to process sequential data by retaining memory over time. Specifically in this tutorial, we train an RNN to perform a **next-step prediction task** on a self-supervised task.\n",
    "\n",
    "What is next-step prediction and why?\n",
    "\n",
    "\n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "* Completed Notebook 1\n",
    "\n",
    "Before starting this notebook, make sure you have:\n",
    "- trajectory data from part 1, including speed and rotational speed\n",
    "- vision data (frames) from part 1\n",
    "- embedded vision data from the Vision Autoencoder we trained in part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2d819",
   "metadata": {},
   "source": [
    "### **0. Install and import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14611fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27b2ed",
   "metadata": {},
   "source": [
    "### **1. Load trajectory and vision embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d7ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our trajectories are at 10 FPS, we want to subsample them to 1 FPS\n",
    "STRIDE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228a60d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed01'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed02'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed03'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed04'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed05'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed06'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed07'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed08'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed09'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed10'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed11'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed12'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed13'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed14'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed15'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed16'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed17'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed18'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed19'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed20'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed21'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed22'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed23'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed24'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed25'),\n",
       " PosixPath('../data/adult/exp_dim0.635_fps10_s720_seed26')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = '../data/adult'\n",
    "trial_paths = sorted([p for p in Path(d).iterdir() if 'exp' in p.name])\n",
    "trial_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "223262f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiple_subsampling(data, stride, is_velocity=False):\n",
    "    new_length = data.shape[0]//stride if not is_velocity else data.shape[0]//stride-1\n",
    "    data_multisubs = np.zeros(\n",
    "        (stride, new_length, data.shape[1]),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    for start_idx in range(stride):\n",
    "        if is_velocity:\n",
    "            if start_idx < stride-1:\n",
    "                data_multisubs[start_idx] = data[start_idx+1:start_idx-stride+1].reshape(\n",
    "                    new_length, stride, -1\n",
    "                ).sum(axis=1)\n",
    "            else:\n",
    "                data_multisubs[start_idx] = data[start_idx+1:].reshape(\n",
    "                    new_length, stride, -1\n",
    "                ).sum(axis=1)\n",
    "        else:\n",
    "            data_multisubs[start_idx] = data[start_idx::stride]\n",
    "    return data_multisubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc1e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = []\n",
    "train_vel, train_rotvel, train_pos, train_hds = [], [], [], []\n",
    "\n",
    "for idx in range(20):\n",
    "    tp = trial_paths[idx]\n",
    "    train_embeddings.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'vision_embeddings.npy'), stride=STRIDE)\n",
    "    )\n",
    "    train_vel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'velocities.npy'), stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    train_rotvel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'rot_velocities.npy')[..., None],\n",
    "            stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    train_pos.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'positions.npy'), stride=STRIDE)\n",
    "    )\n",
    "    train_hds.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'thetas.npy')[..., None], stride=STRIDE)\n",
    "    )\n",
    "\n",
    "train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
    "train_vel = np.concatenate(train_vel, axis=0)\n",
    "train_rotvel = np.concatenate(train_rotvel, axis=0)\n",
    "train_pos = np.concatenate(train_pos, axis=0)\n",
    "train_hds = np.concatenate(train_hds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d120d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "test_vel, test_rotvel, test_pos, test_hds = [], [], [], []\n",
    "\n",
    "for idx in range(20, 23):\n",
    "    tp = trial_paths[idx]\n",
    "    test_embeddings.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'vision_embeddings.npy'), stride=STRIDE)\n",
    "    )\n",
    "    test_vel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'velocities.npy'), stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    test_rotvel.append(\n",
    "        create_multiple_subsampling(\n",
    "            np.load(tp / 'riab_simulation' / 'rot_velocities.npy')[..., None],\n",
    "            stride=STRIDE, is_velocity=True\n",
    "        )\n",
    "    )\n",
    "    test_pos.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'positions.npy'), stride=STRIDE)\n",
    "    )\n",
    "    test_hds.append(\n",
    "        create_multiple_subsampling(np.load(tp / 'riab_simulation' / 'thetas.npy')[..., None], stride=STRIDE)\n",
    "    )\n",
    "\n",
    "test_embeddings = np.concatenate(test_embeddings, axis=0)\n",
    "test_vel = np.concatenate(test_vel, axis=0)\n",
    "test_rotvel = np.concatenate(test_rotvel, axis=0)\n",
    "test_pos = np.concatenate(test_pos, axis=0)\n",
    "test_hds = np.concatenate(test_hds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "783048a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (200, 720, 100)\n",
      "Train velocities shape: (200, 719, 2)\n",
      "Train rotational velocities shape: (200, 719, 1)\n",
      "Train positions shape: (200, 720, 2)\n",
      "Train head directions shape: (200, 720, 1)\n",
      "\n",
      "Test embeddings shape: (30, 720, 100)\n",
      "Test velocities shape: (30, 719, 2)\n",
      "Test rotational velocities shape: (30, 719, 1)\n",
      "Test positions shape: (30, 720, 2)\n",
      "Test head directions shape: (30, 720, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Train velocities shape: {train_vel.shape}\")\n",
    "print(f\"Train rotational velocities shape: {train_rotvel.shape}\")\n",
    "print(f\"Train positions shape: {train_pos.shape}\")\n",
    "print(f\"Train head directions shape: {train_hds.shape}\")\n",
    "print()\n",
    "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Test velocities shape: {test_vel.shape}\")\n",
    "print(f\"Test rotational velocities shape: {test_rotvel.shape}\")\n",
    "print(f\"Test positions shape: {test_pos.shape}\")\n",
    "print(f\"Test head directions shape: {test_hds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2374e24",
   "metadata": {},
   "source": [
    "### **2. Define dataloader**\n",
    "\n",
    "To allow our RNN to perform a next-step prediction task, we can use a **Dataset** and **DataLoader** to make our life easier. The **Dataset** prepares sensory embeddings as well as motion signals to batches of paired inputs and labels; the **DataLoader** sequentially generates these batches during training and testing.\n",
    "\n",
    "The Dataset and Dataloader classes will inherit PyTorch's built-in ```torch.utils.data.Dataset``` and ```torch.utils.data.DataLoader``` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "465ecaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensoryDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, embs, vels, rot_vels, pos, hds, tsteps=9):\n",
    "        '''\n",
    "        The initialisation function for the SensoryDataset class.\n",
    "        At initialisation, all embeddings are converted to tensors.\n",
    "        Args:\n",
    "            embs: The visual embeddings of shape (N, T, D)\n",
    "            vels: The speed signals of shape (N, T-1, 1)\n",
    "            rot_vels: The rotational velocities of shape (N, T-1, 1)\n",
    "            pos: The positions of shape (N, T, 2)\n",
    "            hds: The headings of shape (N, T, 1)\n",
    "            tsteps: The number of time steps for each batch.\n",
    "                By default, this is set to 9 i.e. we use the sensory input from steps 1 to 9          \n",
    "        '''\n",
    "        self.embs = torch.from_numpy(embs)\n",
    "        self.vels = torch.from_numpy(vels)\n",
    "        self.rot_vels = torch.from_numpy(rot_vels)\n",
    "        self.pos = torch.from_numpy(pos)\n",
    "        self.hds = torch.from_numpy(hds)\n",
    "        \n",
    "        self.tsteps = tsteps\n",
    "    \n",
    "    def __len__(self):\n",
    "        # COMPLETE THE CODE HERE: how many samples are in the dataset?\n",
    "        return self.embs.shape[1] // self.tsteps - 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        This function returns a batch of sensory inputs and the corresponding future sensory inputs.\n",
    "        Args:\n",
    "            idx: The index of the sample to return. idx will be automatically generated by the DataLoader.\n",
    "        Returns:\n",
    "        \n",
    "        '''\n",
    "        vels, rot_vels, pos, hds, embs_labels = [], [], [], [], []\n",
    "\n",
    "        start_idx, end_idx = idx*self.tsteps, (idx + 1)*self.tsteps\n",
    "\n",
    "        embs = self.embs[:, start_idx:end_idx]\n",
    "\n",
    "        vels = self.vels[:, start_idx:end_idx]\n",
    "        rot_vels = self.rot_vels[:, start_idx:end_idx]\n",
    "        pos = self.pos[:, start_idx:end_idx]\n",
    "        hds = self.hds[:, start_idx:end_idx]\n",
    "\n",
    "        embs_labels = self.embs[:, start_idx+1 : end_idx+1]\n",
    "        \n",
    "        return embs, vels, rot_vels, pos, hds, embs_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af041e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very important: we do not shuffle the datasets because\n",
    "# hidden states are dependent on the previous batch\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    SensoryDataset(\n",
    "        train_embeddings, train_vel, train_rotvel, train_pos, train_hds\n",
    "    ), shuffle=False\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    SensoryDataset(\n",
    "        test_embeddings, test_vel, test_rotvel, test_pos, test_hds\n",
    "    ), shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "830544ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the batch (1, BATCH_SIZE, TIMESTEPS, N_FEATURES)\n",
      "\n",
      "Embeddings:\t\ttorch.Size([1, 200, 9, 100])\n",
      "Velocities:\t\ttorch.Size([1, 200, 9, 2])\n",
      "Rot. velocities:\ttorch.Size([1, 200, 9, 1])\n",
      "Positions:\t\ttorch.Size([1, 200, 9, 2])\n",
      "Head directions:\ttorch.Size([1, 200, 9, 1])\n",
      "Embeddings labels:\ttorch.Size([1, 200, 9, 100])\n"
     ]
    }
   ],
   "source": [
    "for b in train_dataloader:\n",
    "    embs, vels, rot_vels, pos, hds, embs_labels = b\n",
    "    print('Shape of the batch (1, BATCH_SIZE, TIMESTEPS, N_FEATURES)')\n",
    "    print()\n",
    "    print(f'Embeddings:\\t\\t{embs.shape}')\n",
    "    print(f'Velocities:\\t\\t{vels.shape}')\n",
    "    print(f'Rot. velocities:\\t{rot_vels.shape}')\n",
    "    print(f'Positions:\\t\\t{pos.shape}')\n",
    "    print(f'Head directions:\\t{hds.shape}')\n",
    "    print(f'Embeddings labels:\\t{embs_labels.shape}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369ff46f",
   "metadata": {},
   "source": [
    "### **3. Define the RNN**\n",
    "\n",
    "Let's define the RNN that will serve as out hippocampus. This network will use a customized ```RNNModule``` class, which you can find in `rnn/rnn_module.py`, that applies a Sigmoid activation function to the hidden state. The hidden states are projected to predict the next sensory state via a linear layer.\n",
    "\n",
    "The output of the Sigmoid sits between 0 and 1, which can be interpreted as the (scaled) firing rate (or simply activity) of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "327b30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, input_bias, hidden_bias):\n",
    "        super(RNNCell, self).__init__()\n",
    "\n",
    "        self.in2hidden = torch.nn.Linear(n_inputs, n_hidden, bias=input_bias)\n",
    "        self.hidden2hidden = torch.nn.Linear(n_hidden, n_hidden, bias=hidden_bias)\n",
    "\n",
    "        self.activation_fn = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        igates = self.in2hidden(x)\n",
    "        hgates = self.hidden2hidden(hidden)\n",
    "        return self.activation_fn(igates + hgates)\n",
    "\n",
    "\n",
    "class RNNModule(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, device, n_inputs, n_hidden,\n",
    "        input_bias, hidden_bias\n",
    "    ):\n",
    "        super(RNNModule, self).__init__()\n",
    "\n",
    "        self.rnn_cell = RNNCell(n_inputs, n_hidden, input_bias, hidden_bias)\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [BATCH SIZE, TIME, N_FEATURES]\n",
    "        # hidden: [BATCH SIZE, N_HIDDEN]\n",
    "        \n",
    "        output = torch.zeros(x.shape[0], x.shape[1], self.n_hidden).to(self.device)\n",
    "\n",
    "        if hidden is None:\n",
    "            h_out = torch.zeros(x.shape[0], self.n_hidden) # initialize hidden state\n",
    "            h_out = h_out.to(self.device)\n",
    "        else:\n",
    "            h_out = hidden\n",
    "\n",
    "        window_size = x.shape[1]\n",
    "\n",
    "        # loop over time\n",
    "        for t in range(window_size):\n",
    "            x_t = x[:,t,...]\n",
    "            h_out = self.rnn_cell(x_t, h_out)\n",
    "            output[:,t,...] = h_out\n",
    "\n",
    "        # return all outputs, and the last hidden state\n",
    "        return output, h_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictiveRNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        device, n_inputs, n_hidden, n_outputs, bias=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rnn = RNNModule(\n",
    "            device, n_inputs, n_hidden,\n",
    "            input_bias=bias, hidden_bias=bias\n",
    "        )\n",
    "\n",
    "        self.linear_layer = torch.nn.Linear(n_hidden, n_outputs, bias=bias)\n",
    "\n",
    "    def inputs2hidden(self, inputs, hidden):\n",
    "        \"\"\" Encodes the input tensor into a latent representation.\n",
    "\n",
    "        Args:\n",
    "            x: [BATCH SIZE, TIME, CHANNELS, HEIGHT, WIDTH]\n",
    "        \"\"\"\n",
    "        \n",
    "        if hidden is not None:\n",
    "            return self.rnn(inputs, hidden[None, ...])[0]\n",
    "        else:\n",
    "            return self.rnn(inputs)[0]\n",
    "\n",
    "    def hidden2outputs(self, hidden):\n",
    "        return self.linear_layer(hidden)\n",
    "    \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        hidden_new = self.inputs2hidden(inputs, hidden)\n",
    "\n",
    "        output = self.hidden2outputs(hidden_new)\n",
    "\n",
    "        return output, hidden_new[:,-1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229dde1",
   "metadata": {},
   "source": [
    "Note that the input dimension would be both the visual embedding dimension and the motion signal, while the output dimension would be only the visual embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7a46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_embedding_dim = train_embeddings.shape[-1]\n",
    "motion_signal_dim = train_vel.shape[-1] + train_rotvel.shape[-1]\n",
    "\n",
    "rnn = PredictiveRNN(\n",
    "    DEVICE,\n",
    "    n_inputs=visual_embedding_dim + motion_signal_dim,\n",
    "    n_hidden=500,\n",
    "    n_outputs=visual_embedding_dim\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da5fe6",
   "metadata": {},
   "source": [
    "### **4. Train RNN on self-supervised predictive task**\n",
    "\n",
    "Training the RNN is similar to training the autoencoder, except that we need to **pass the hidden state from one batch to the next**, it's important to note that batches are now sequential rather than independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbfa5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    rnn,\n",
    "    dataloader,\n",
    "    loss_fn, optimizer\n",
    "):\n",
    "    '''\n",
    "    This function trains the RNN for one epoch. \n",
    "    We will default to only predcting the next step sensory input,\n",
    "    i.e. n_future_pred = 1.\n",
    "    '''\n",
    "    rnn.train()\n",
    "    \n",
    "    batch_losses = []\n",
    "    hidden_state = None # Initialize hidden state to zeros\n",
    "\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embs, vels, rot_vels, _, _, embs_labels = batch\n",
    "        \n",
    "        # COMPLETE THE CODE HERE:\n",
    "        inputs = torch.cat((\n",
    "            embs.squeeze(dim=0).to(DEVICE),\n",
    "            vels.squeeze(dim=0).to(DEVICE),\n",
    "            rot_vels.squeeze(dim=0).to(DEVICE)\n",
    "        ), dim=-1)\n",
    "\n",
    "        outputs, hidden_new = rnn(inputs, hidden_state)\n",
    "\n",
    "        embs_labels = embs_labels.squeeze(dim=0).to(DEVICE)\n",
    "\n",
    "        # compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, embs_labels)\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(rnn.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_losses.append(loss.detach().item())\n",
    "\n",
    "        # Assign new RNN hidden state to variable.\n",
    "        # Detach it to prevent backpropagation\n",
    "        # through the entire history\n",
    "        hidden_state = hidden_new.detach()\n",
    "\n",
    "    return batch_losses "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8a3d2",
   "metadata": {},
   "source": [
    "A paired test function has been provided for convenience. This function additionally saves the hidden state at each step. This is needed for computing the rate maps in the later parts of this tutorial. It is structured like this:\n",
    "\n",
    "```python\n",
    "def test_rnn_epoch(rnn, dataloader, loss_fn, stepwise=True):\n",
    "    ...\n",
    "    return test_dict    \n",
    "```\n",
    "\n",
    "Where, ```test_dict``` is a dictionary containing the following keys:\n",
    "* ```batch_losses```;\n",
    "* ```h_ts```, the hidden state at each time step;\n",
    "* ```pos```, the (x, y) position of the agent at each time step;\n",
    "* ```hds``, the (dx, dy) direction of the agent at each time step as a unit circle;\n",
    "* ```thetas```, the head bearing of the agent at each time step.\n",
    "\n",
    "This function will be imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fab6d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae62ad",
   "metadata": {},
   "source": [
    "### 4 Putting it all together\n",
    "\n",
    "We will define some parameters for training, then train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e98b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some training parameters\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "init_lr = 1e-3\n",
    "optimizer = torch.optim.RMSprop(rnn.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "n_epochs = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "104cc1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n",
      "[(30, 711, 500), (30, 711, 2), (30, 711, 1), (30, 711, 100), (30, 711, 100)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m test_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m d \u001b[38;5;241m=\u001b[39m evaluate_rnn(DEVICE, rnn, test_dataloader, loss_fn, for_ratemaps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m([d[k]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhead_directions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membs_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[0;32mIn[14], line 36\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(rnn, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(rnn.parameters(), 1.0)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m batch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Assign new RNN hidden state to variable.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Detach it to prevent backpropagation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# through the entire history\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:124\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    123\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/rmsprop.py:175\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    163\u001b[0m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    165\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    166\u001b[0m         group,\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m         state_steps,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m     \u001b[43mrmsprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcentered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tutorial/lib/python3.10/site-packages/torch/optim/rmsprop.py:511\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, state_steps, foreach, maximize, differentiable, capturable, has_complex, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    509\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rmsprop\n\u001b[0;32m--> 511\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_means, test_means = [], []\n",
    "train_stds, test_stds = [], []\n",
    "epochs = []\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # COMPLETE THE CODE HERE: train the RNN for one epoch, then test it\n",
    "    train_losses = None\n",
    "    test_dict = None\n",
    "    \n",
    "    train_losses = train_epoch(rnn, train_dataloader, loss_fn, optimizer)\n",
    "    d = evaluate_rnn(DEVICE, rnn, test_dataloader, loss_fn, for_ratemaps=True)\n",
    "    \n",
    "    print([d[k].shape for k in ['hidden_states', 'positions', 'head_directions', 'outputs', 'embs_labels']])\n",
    "\n",
    "    # test_losses = d['batch_losses']\n",
    "    \n",
    "    # train_mean, train_std = np.mean(train_losses), np.std(train_losses)\n",
    "    # test_mean, test_std = np.mean(test_losses), np.std(test_losses)\n",
    "\n",
    "    # train_means.append(train_mean)\n",
    "    # train_stds.append(train_std)\n",
    "    # test_means.append(test_mean)\n",
    "    # test_stds.append(test_std)\n",
    "    # epochs.append(epoch)\n",
    "\n",
    "    # # Clear previous plot output\n",
    "    # clear_output(wait=True)\n",
    "    \n",
    "    # # Create new plot\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "    # plt.plot(epochs, train_means, label='Train Loss', color='blue')\n",
    "    # plt.fill_between(\n",
    "    #     epochs,\n",
    "    #     np.array(train_means) - np.array(train_stds),\n",
    "    #     np.array(train_means) + np.array(train_stds),\n",
    "    #     color='blue', alpha=0.3\n",
    "    # )\n",
    "    # plt.plot(epochs, test_means, label='Test Loss', color='orange')\n",
    "    # plt.fill_between(\n",
    "    #     epochs,\n",
    "    #     np.array(test_means) - np.array(test_stds),\n",
    "    #     np.array(test_means) + np.array(test_stds),\n",
    "    #     color='orange', alpha=0.3\n",
    "    # )\n",
    "    # plt.xlabel(\"Epoch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.title(\"Train vs Test Loss\")\n",
    "    # plt.legend()\n",
    "    # plt.grid(True)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863aa55",
   "metadata": {},
   "source": [
    "### **What have we achieved in this tutorial?**\n",
    "\n",
    "We trained a recurrent neural network to perform the next-step prediction task.\n",
    "\n",
    "In the next tutorial, we will look into how to extract realistic place cell rate maps from this RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac2c77b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
